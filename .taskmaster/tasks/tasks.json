{
  "master": {
    "tasks": [
      {
        "id": 1,
        "title": "Enhance AuthDialog for OpenRouter API Key Prompt",
        "description": "Modify AuthDialog.tsx to prompt for OpenRouter API key when selected, ensuring secure handling and environment/config storage.",
        "details": "Update /packages/cli/src/ui/components/AuthDialog.tsx to include a prompt for OpenRouter API key input. Use secure input handling (e.g., masking). Store the API key in environment variables or a secure config file (e.g., dotenv for Node.js, or a custom encrypted config). Check for existing API keys in environment variables and skip prompt if present. Use react-hook-form for form state management (v7+).",
        "testStrategy": "Test that the prompt appears only for OpenRouter selection. Verify API key is stored securely and not logged. Test environment variable override. Manual and automated UI tests.",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 2,
        "title": "Implement API Key Validation and Error Handling",
        "description": "Add logic to validate OpenRouter API keys and provide user feedback for invalid or missing keys.",
        "details": "In /packages/cli/src/ui/hooks/useAuthCommand.ts, implement API key validation by making a test request to OpenRouter (e.g., GET /api/keys endpoint). Use axios (v1.6+) for HTTP requests. Display clear error messages for invalid or missing keys. Handle rate limits and API errors gracefully. Store validated API key securely.",
        "testStrategy": "Test with valid, invalid, and missing API keys. Verify error messages and graceful handling. Automated integration tests for API key validation.",
        "priority": "high",
        "dependencies": [
          1
        ],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 3,
        "title": "Add Model Listing and Selection UI",
        "description": "Implement UI to list available OpenRouter models and allow user selection, consistent with existing auth patterns.",
        "details": "Extend AuthDialog.tsx or create a new ModelSelection component to list models fetched from OpenRouter (GET /api/models). Use react-select (v5+) for dropdown. Save selected model in state and config. Ensure UI matches existing sprtscltr patterns. Use react-query (v4+) for data fetching and caching.",
        "testStrategy": "Test model listing and selection. Verify UI consistency. Manual and automated UI tests.",
        "priority": "medium",
        "dependencies": [
          2
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Fetch available models from OpenRouter API using react-query",
            "description": "Implement data fetching logic to retrieve available models from the OpenRouter API, utilizing react-query for caching and state management. Ensure the query key includes relevant parameters such as baseUrl for proper cache invalidation.",
            "dependencies": [],
            "details": "Use the useOpenRouterModelProviders hook or similar, passing the custom baseUrl from user configuration. Ensure the queryKey includes baseUrl as part of its key for correct caching and refetching when the endpoint changes.\n<info added on 2025-07-08T05:29:52.661Z>\nPlan update:\n- Implement a fetchModels method within OpenRouterContentGenerator to retrieve available models from the configured endpoint.\n- Create a useOpenRouterModels hook that uses fetchModels to fetch and locally cache the model list, ensuring efficient reuse and updates when the baseUrl changes.\n- Refactor ModelSelector to consume the models provided by useOpenRouterModels instead of relying on hardcoded values.\n- Add logic to ModelSelector to display loading indicators while fetching and handle error states gracefully if model retrieval fails.\n</info added on 2025-07-08T05:29:52.661Z>\n<info added on 2025-07-08T05:31:25.764Z>\nCreated fetchModels method in OpenRouterContentGenerator to retrieve models from the /api/v1/models endpoint using the configured baseUrl. Developed useOpenRouterModels React hook with a 5-minute cache TTL and integrated error handling. Updated ModelSelector to consume the dynamic model list from useOpenRouterModels, replacing the previous hardcoded values. Implemented loading indicators and error state handling in ModelSelector to improve user experience during model retrieval.\n</info added on 2025-07-08T05:31:25.764Z>",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Implement or extend ModelSelection component",
            "description": "Create a new ModelSelection component or extend the existing AuthDialog.tsx to include model selection functionality, integrating the data fetched in the previous step.",
            "dependencies": [
              1
            ],
            "details": "Ensure the component receives the list of models as props or via hook, and is structured to allow for easy integration of a dropdown UI.\n<info added on 2025-07-08T05:34:52.821Z>\nIntegrated the ModelSelector component into the main App.tsx rendering flow, utilizing the useModelSelector hook for managing model selection state. Implemented a /model slash command that triggers the model selector UI, which is only accessible when authenticated with OpenRouter. Ensured all components are properly connected with state management so that the model selector appears dynamically upon typing the /model command.\n</info added on 2025-07-08T05:34:52.821Z>",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Integrate react-select (v5+) for dropdown UI",
            "description": "Replace or enhance the model selection UI with react-select (v5+) to provide a user-friendly dropdown for model selection.",
            "dependencies": [
              2
            ],
            "details": "Configure react-select with the fetched model data, ensuring accessibility and usability. Style the dropdown to match existing UI patterns.\n<info added on 2025-07-08T05:38:46.371Z>\nRadioButtonSelect will be retained for model selection to maintain UI consistency with other dialogs (AuthDialog, ThemeDialog, EditorDialog). The ModelSelector component already provides a dropdown interface with loading, error, and keyboard accessibility features. Introducing react-select is unnecessary and would disrupt the established UI pattern while adding extra dependencies.\n</info added on 2025-07-08T05:38:46.371Z>",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Persist selected model in state and config",
            "description": "Ensure that the user's selected model is persisted both in component state and in the application's configuration, so it remains consistent across sessions and reloads.",
            "dependencies": [
              3
            ],
            "details": "Update state management logic to store the selected model, and synchronize with user configuration or context as appropriate.\n<info added on 2025-07-08T05:39:55.856Z>\nThe selected model is currently persisted for the session using config.setModel() within handleModelSelect, aligning with the established pattern where model selection is determined by environment variables (OPENROUTER_MODEL) or defaults. Persisting the selection across sessions would require writing to settings.json, but this is not implemented as it would diverge from the current approach. No changes to cross-session persistence are needed unless the requirements change.\n</info added on 2025-07-08T05:39:55.856Z>",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Ensure UI consistency and write UI tests",
            "description": "Review the implementation for consistency with sprtscltr UI patterns and write both manual and automated UI tests to verify correct behavior and appearance.",
            "dependencies": [
              4
            ],
            "details": "Perform visual and functional checks, update styles as needed, and implement tests using the project's preferred testing framework.\n<info added on 2025-07-08T05:41:35.873Z>\nUI consistency has been ensured by standardizing on the RadioButtonSelect component, mirroring patterns from AuthDialog, ThemeDialog, and EditorDialog, and implementing loading spinners, error states, and keyboard navigation (Enter to select, Escape to cancel). Styling uses Colors constants for visual alignment. Tests have been written for ModelSelector, useOpenRouterModels, and useModelSelector, following project conventions, though dependency issues currently prevent execution.\n</info added on 2025-07-08T05:41:35.873Z>",
            "status": "done",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 4,
        "title": "Enable Model Switching and Persistence",
        "description": "Model switching is already implemented: users can switch models mid-session using the /model command, and the model is persisted for the current session via config.setModel(). The UI displays a model change notification. The implementation follows the existing pattern where the model is determined by environment variables. Cross-session persistence (e.g., saving to settings.json) is not implemented, as this would deviate from the current architecture where auth-related settings come from environment variables.",
        "status": "done",
        "dependencies": [
          3
        ],
        "priority": "medium",
        "details": "No further changes are required for mid-session model switching or session persistence. Cross-session persistence (saving model preference to settings.json) is not implemented to maintain consistency with the current architecture, which relies on environment variables for auth and model configuration. Document this architectural decision and ensure user documentation reflects the current behavior and limitations.",
        "testStrategy": "Verify that users can switch models mid-session using the /model command and that the selected model persists for the duration of the session. Confirm that the UI displays a notification when the model changes. Automated integration tests should cover these behaviors. No tests are required for cross-session persistence, as it is not supported.",
        "subtasks": [
          {
            "id": 1,
            "title": "Document current model switching and persistence behavior",
            "description": "Update user and developer documentation to clarify that model switching is supported mid-session and persists for the session, but cross-session persistence is not implemented due to architectural constraints.",
            "status": "done",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Add automated integration tests for model switching and session persistence",
            "description": "Ensure tests cover switching models with /model command, session persistence, and UI notifications. No tests for cross-session persistence are needed.",
            "status": "done",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 5,
        "title": "Display Current Model in UI",
        "description": "Show the currently selected OpenRouter model in the status bar or header.",
        "details": "Update /packages/cli/src/ui/App.tsx to display the current model. Use context or state management (e.g., Zustand v4+ or React Context) to share model state across components. Ensure display updates on model switch.",
        "testStrategy": "Test that current model is displayed and updates on switch. Manual and automated UI tests.",
        "priority": "medium",
        "dependencies": [
          4
        ],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 6,
        "title": "Ensure Backward Compatibility and Provider Consistency",
        "description": "Maintain compatibility with existing auth methods and ensure custom API providers follow the same pattern.",
        "details": "Review and update all auth and model selection flows to ensure backward compatibility. Apply the same model switching pattern to other providers if supported. Use TypeScript for type safety. Document changes and update configuration schema.",
        "testStrategy": "Test all auth methods and providers. Verify backward compatibility and consistent behavior. Automated integration and regression tests.",
        "priority": "medium",
        "dependencies": [
          5
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 7,
        "title": "Fix OpenRouter/DeepSeek Streaming Output Issue",
        "description": "Resolve the issue where the response is being built character by character, causing repeated text in the streaming output from OpenRouter/DeepSeek API.",
        "details": "To address this issue, implement the following steps: \n1. **Review API Request Structure**: Ensure that the API request to OpenRouter/DeepSeek is correctly formatted to handle streaming responses. This may involve setting the `stream` parameter to `true` in the API call, as shown in the DeepSeek API documentation[1]. \n2. **Buffering and Response Handling**: Implement a buffering mechanism to collect the streaming response in chunks rather than processing it character by character. This can be achieved using a library like `async-iterator` in Node.js or similar constructs in other languages. \n3. **Error Handling and Logging**: Enhance error handling to detect and log any issues that might cause repeated text, such as network errors or API rate limits. Use logging libraries to track these events for debugging purposes. \n4. **Testing with Different Models**: Test the streaming functionality with various models available through OpenRouter to ensure the fix is model-agnostic. \n5. **Code Refactoring**: Refactor the code to improve readability and maintainability, ensuring that the streaming logic is modular and easy to update. \n\nExample code for handling streaming responses in Node.js might look like this: \n```javascript\nimport { createReadStream } from 'fs';\nimport axios from 'axios';\n\nconst apiStream = async () => {\n  const response = await axios.get('https://api.deepseek.com/chat/completions', {\n    params: { stream: true },\n    responseType: 'stream'\n  });\n\n  const chunks = [];\n  response.data.on('data', chunk => chunks.push(chunk));\n  response.data.on('end', () => {\n    const fullResponse = Buffer.concat(chunks).toString();\n    console.log(fullResponse);\n  });\n};\n```\n",
        "testStrategy": "1. **Unit Tests**: Write unit tests to verify that the streaming response is correctly buffered and processed without repeated text. Use mocking libraries to simulate API responses. \n2. **Integration Tests**: Conduct integration tests with the OpenRouter/DeepSeek API to ensure the fix works in real-world scenarios. Test with different models and edge cases (e.g., network errors). \n3. **Manual Testing**: Perform manual testing to visually inspect the output for any issues. \n4. **Performance Testing**: Run performance tests to ensure that the buffering mechanism does not introduce significant latency or memory usage issues.",
        "status": "done",
        "dependencies": [
          1,
          2,
          6
        ],
        "priority": "medium",
        "subtasks": []
      },
      {
        "id": 8,
        "title": "Fix Token Usage Tracking for OpenRouter/DeepSeek Models",
        "description": "Update the system to accurately track and display token usage for OpenRouter/DeepSeek models, ensuring usage stats reflect actual tokens consumed.",
        "details": "Investigate the current implementation for tracking token usage with OpenRouter/DeepSeek models, focusing on why token counts are reported as zero. Review the API responses from OpenRouter/DeepSeek to determine if token usage information is returned (e.g., in response metadata or headers). If the API does not provide token usage directly, implement logic to estimate tokens based on the prompt and completion using a compatible tokenizer (such as tiktoken or a DeepSeek-specific tokenizer). Integrate this logic into the model usage stats pipeline, ensuring that token counts are updated and displayed correctly in the UI and any relevant logs or analytics. If using a third-party analytics or logging tool (e.g., Langfuse), ensure custom model definitions are set up to enable token cost tracking, as described in community discussions and documentation. Document any changes to the tracking logic and update configuration or environment variables as needed.",
        "testStrategy": "1. Unit test the token counting logic with a variety of prompts and completions to ensure accuracy. 2. Simulate API responses from OpenRouter/DeepSeek and verify that token usage is correctly parsed or estimated. 3. Perform integration tests to confirm that token usage stats are updated in the UI and logs after model invocations. 4. If using analytics tools, verify that token usage and cost are reported as expected for OpenRouter/DeepSeek models. 5. Conduct regression tests to ensure token tracking for other providers remains unaffected.",
        "status": "done",
        "dependencies": [
          1,
          7
        ],
        "priority": "medium",
        "subtasks": []
      },
      {
        "id": 9,
        "title": "Implement Model Discovery Feature",
        "description": "Add --models CLI flag and /models slash command to display available AI models grouped by provider, showing configuration status and model capabilities",
        "details": "",
        "testStrategy": "",
        "status": "pending",
        "dependencies": [
          6
        ],
        "priority": "high",
        "subtasks": [
          {
            "id": 1,
            "title": "Define IProvider Interface with get_available_models()",
            "description": "Create a new IProvider interface that standardizes model discovery by requiring a get_available_models() method for all providers.",
            "dependencies": [],
            "details": "Design the IProvider interface in the shared provider module. The interface should specify a get_available_models() method that returns a list of model metadata objects, including model name, capabilities, and configuration status.",
            "status": "done",
            "testStrategy": "Write unit tests to ensure that any class implementing IProvider must define get_available_models()."
          },
          {
            "id": 2,
            "title": "Implement get_available_models() for Gemini Provider",
            "description": "Add the get_available_models() method to the Gemini provider, returning all supported models with their metadata and configuration status.",
            "dependencies": [
              1
            ],
            "details": "In the Gemini provider implementation, fetch or define the list of available models. For each model, include metadata such as description, context window, pricing, and configuration status (e.g., API key present).",
            "status": "done",
            "testStrategy": "Test that get_available_models() returns accurate model data and correct status based on configuration."
          },
          {
            "id": 3,
            "title": "Implement get_available_models() for OpenRouter Provider",
            "description": "Add the get_available_models() method to the OpenRouter provider, returning all supported models with their metadata and configuration status.",
            "dependencies": [
              1
            ],
            "details": "In the OpenRouter provider, fetch the model list via API or static config. Include metadata (description, context window, pricing) and check for valid API key to set configuration status.",
            "status": "done",
            "testStrategy": "Test with valid and invalid API keys to verify correct model listing and status reporting."
          },
          {
            "id": 4,
            "title": "Implement get_available_models() for Custom API Provider",
            "description": "Add the get_available_models() method to the Custom API provider, returning all supported models with their metadata and configuration status.",
            "dependencies": [
              1
            ],
            "details": "For the Custom API provider, define how models are discovered (e.g., static config or API call). Return model metadata and indicate if required configuration (e.g., endpoint, key) is present.",
            "status": "done",
            "testStrategy": "Test with various custom API configurations to ensure correct model discovery and status."
          },
          {
            "id": 5,
            "title": "Add --models CLI Argument Parsing",
            "description": "Extend the CLI parser to recognize the --models flag and trigger model discovery logic.",
            "dependencies": [
              2,
              3,
              4
            ],
            "details": "Update the CLI argument parser to handle --models. When invoked, aggregate model lists from all providers using their get_available_models() methods.",
            "status": "pending",
            "testStrategy": "Test CLI with --models flag to ensure it triggers model discovery and handles errors gracefully."
          },
          {
            "id": 6,
            "title": "Create Model Listing Output Formatter",
            "description": "Develop a formatter to display models grouped by provider, showing configuration status, capabilities, and metadata.",
            "dependencies": [
              5
            ],
            "details": "Implement a function to format the aggregated model data. Group models by provider, display configuration status (e.g., ✅/❌), and show metadata such as description, context window, and pricing. Ensure output is readable in both CLI and interactive modes.",
            "status": "pending",
            "testStrategy": "Test output formatting with various provider/model combinations and missing configurations."
          },
          {
            "id": 7,
            "title": "Implement /models Slash Command for Interactive Mode",
            "description": "Add a /models command to the interactive CLI that invokes the model discovery logic and displays formatted output.",
            "dependencies": [],
            "details": "Register the /models command in the interactive CLI. When triggered, call the model discovery logic and display the formatted model list to the user.",
            "status": "pending",
            "testStrategy": "Test /models command in interactive mode for correct output and error handling."
          },
          {
            "id": 8,
            "title": "Add Model Aliases Configuration and Comprehensive Tests",
            "description": "Support model aliases in configuration and create tests covering all aspects of the model discovery feature.",
            "dependencies": [],
            "details": "Allow users to define aliases for models in the config. Update model discovery logic to display aliases. Write tests for interface, provider implementations, CLI flag, slash command, output formatting, and alias handling.",
            "status": "pending",
            "testStrategy": "Write unit and integration tests for alias resolution, model listing, and all user-facing features."
          }
        ]
      },
      {
        "id": 10,
        "title": "Enhance Shell Tool Transparency",
        "description": "Improve ShellTool output to show executed command, exit code, full stdout/stderr, and execution duration for better debugging and user confidence",
        "details": "",
        "testStrategy": "",
        "status": "pending",
        "dependencies": [
          9
        ],
        "priority": "medium",
        "subtasks": [
          {
            "id": 1,
            "title": "Modify ShellTool Return Structure",
            "description": "Update the ShellTool's return structure to include the executed command, exit code, full stdout, and stderr outputs.",
            "dependencies": [],
            "details": "Refactor the ShellTool's execution logic so that every run returns a structured object containing the command string, its exit code, and the complete standard output and error streams.",
            "status": "pending",
            "testStrategy": "Write unit tests to verify that the returned object always includes all required fields with accurate values for various command scenarios."
          },
          {
            "id": 2,
            "title": "Implement Execution Duration Tracking",
            "description": "Add logic to measure and record the duration of each command execution within the ShellTool.",
            "dependencies": [
              1
            ],
            "details": "Integrate timing functionality to capture the start and end time of each command, calculating the total execution duration and including it in the return structure.",
            "status": "pending",
            "testStrategy": "Test with commands of varying lengths to ensure the duration is measured accurately and consistently included in the output."
          },
          {
            "id": 3,
            "title": "Update Output Formatting",
            "description": "Revise the ShellTool's output formatting to clearly display the executed command, exit code, stdout, stderr, and execution duration.",
            "dependencies": [
              2
            ],
            "details": "Design and implement a user-friendly output format that presents all transparency details in a readable and organized manner, suitable for both CLI and potential UI consumption.",
            "status": "pending",
            "testStrategy": "Verify output formatting through snapshot and integration tests, ensuring all fields are present and clearly labeled."
          },
          {
            "id": 4,
            "title": "Add Verbosity Settings",
            "description": "Introduce verbosity settings to allow users to control the level of detail shown in ShellTool output.",
            "dependencies": [
              3
            ],
            "details": "Implement configuration options or command-line flags that let users toggle between minimal and detailed output, affecting which fields are displayed.",
            "status": "pending",
            "testStrategy": "Test all verbosity levels to confirm that output detail matches user settings and that toggling works as expected."
          },
          {
            "id": 5,
            "title": "Create Tests for Enhanced Transparency Features",
            "description": "Develop comprehensive tests to validate all new transparency features and ensure robust, reliable behavior.",
            "dependencies": [
              4
            ],
            "details": "Write unit, integration, and regression tests covering all aspects of the enhanced output, including edge cases and error conditions.",
            "status": "pending",
            "testStrategy": "Automate test execution and require all tests to pass before merging changes, ensuring ongoing reliability."
          }
        ]
      },
      {
        "id": 11,
        "title": "Improve Model Listing with Loading Indicators, Intelligent Organization, and Metadata-Based Summaries",
        "description": "Enhance the model listing feature to display loading indicators, organize models intelligently, and provide helpful summaries and recommendations using OpenRouter's model metadata such as pricing, performance, and use cases.",
        "details": "1. Refactor the model listing UI to show a clear loading indicator while fetching models from OpenRouter, ensuring users are informed of data loading states.\n2. Implement intelligent grouping and sorting of models based on key metadata (e.g., provider, pricing tier, performance benchmarks, recommended use cases). Consider using sections or tabs for major providers or categories.\n3. For each model, display a concise summary panel that highlights pricing, performance metrics, and ideal use cases, leveraging metadata from the OpenRouter API. Use badges or icons for quick visual cues (e.g., 'Best for Coding', 'Low Cost', 'High Accuracy').\n4. Add a recommendation engine that suggests models based on user context or common usage patterns (e.g., if the user often selects coding models, highlight those first).\n5. Ensure accessibility and responsiveness in the updated UI, following existing design patterns.\n6. Update data fetching logic to handle errors gracefully and display appropriate messages if metadata is missing or incomplete.\n7. Refactor or extend the existing ModelSelection component (from Task 3) to incorporate these enhancements, ensuring compatibility with the current model selection and state management flows.",
        "testStrategy": "- Verify that the loading indicator appears during model fetch and disappears once data is loaded.\n- Confirm that models are grouped and sorted as intended, with clear visual separation and accurate metadata display.\n- Check that each model's summary includes pricing, performance, and use case information, and that recommendations are contextually relevant.\n- Test error handling by simulating API failures or missing metadata.\n- Ensure the UI remains accessible and responsive across devices.\n- Perform regression testing to confirm that model selection and configuration continue to work as before.",
        "status": "pending",
        "dependencies": [
          3,
          9
        ],
        "priority": "high",
        "subtasks": []
      },
      {
        "id": 12,
        "title": "Update Branding from Google Gemini CLI to Multi-Provider CLI",
        "description": "Rebrand the entire codebase from \"Google Gemini CLI\" to a multi-provider CLI, updating all references in package.json files, README, documentation, CLI output, and hardcoded strings while maintaining backward compatibility where appropriate.",
        "details": "1. **Package.json Updates**: Update all package.json files across the monorepo to reflect the new multi-provider branding. Change package names, descriptions, and metadata. Consider using a neutral name like \"AI CLI\" or \"Multi-AI CLI\" instead of provider-specific branding.\n\n2. **README and Documentation**: \n   - Update the main README.md and all package-specific READMEs to reflect multi-provider support\n   - Change project title, descriptions, and examples to be provider-agnostic\n   - Update installation instructions and usage examples to showcase multiple providers\n   - Ensure OpenRouter, DeepSeek, and other supported providers are prominently mentioned\n\n3. **CLI Output and Messages**:\n   - Search for all hardcoded strings containing \"Gemini\", \"Google Gemini CLI\", or similar references\n   - Update welcome messages, help text, and error messages to reflect multi-provider nature\n   - Update the CLI banner/logo if present to be provider-neutral\n   - Ensure model selection prompts clearly indicate multiple provider support\n\n4. **Code Comments and Internal References**:\n   - Update code comments that reference Gemini-specific functionality\n   - Rename variables, functions, or classes that have \"gemini\" in their names to be provider-agnostic\n   - Update TypeScript interfaces and types to use generic naming conventions\n\n5. **Backward Compatibility Considerations**:\n   - If the CLI was published under a Gemini-specific package name, consider publishing under a new name while deprecating the old one\n   - Maintain any Gemini-specific environment variables (e.g., GEMINI_API_KEY) alongside new generic ones\n   - Add migration guide for users upgrading from the Gemini-specific version\n   - Consider adding aliases for old commands if command names are changing\n\n6. **Configuration Files**:\n   - Update any default configuration files to reflect multi-provider support\n   - Ensure configuration schemas support multiple providers without bias toward Gemini\n\n7. **Build and CI/CD**:\n   - Update build scripts, GitHub Actions workflows, and other CI/CD configurations\n   - Change repository description and topics on GitHub to reflect multi-provider support",
        "testStrategy": "1. **Comprehensive Text Search**: Use grep or similar tools to search for all instances of \"Gemini\", \"Google Gemini\", and related terms across the entire codebase. Verify no references remain after updates.\n\n2. **Package Installation Test**: Test that the package can be installed under its new name and that all functionality works as expected. If maintaining the old package name for compatibility, test that both work correctly.\n\n3. **CLI Output Verification**: Run all CLI commands and verify that output messages, help text, and error messages reflect the multi-provider branding. Take screenshots of key outputs for documentation.\n\n4. **Documentation Review**: Manually review all documentation files to ensure consistent branding and that examples work with multiple providers, not just Gemini.\n\n5. **Backward Compatibility Tests**: \n   - Test that existing Gemini API key environment variables still work\n   - Verify that any deprecated commands show appropriate warnings\n   - Test migration path from old to new configuration format\n\n6. **Integration Tests**: Run the full test suite to ensure no functionality is broken by the rebranding. Pay special attention to provider selection and authentication flows.\n\n7. **Build and Deploy Test**: Verify that the project builds successfully with new branding and that CI/CD pipelines execute without errors. Test package publishing if applicable.",
        "status": "pending",
        "dependencies": [
          6,
          9
        ],
        "priority": "high",
        "subtasks": [
          {
            "id": 1,
            "title": "Update package.json files across monorepo",
            "description": "Update all package.json files to rebrand from 'gemini-cli' to 'sport-cli', including package names, descriptions, bin commands, and metadata",
            "dependencies": [],
            "details": "Search for all package.json files in the monorepo and update: 1) 'name' field from any variation of 'gemini-cli' to 'sport-cli' or '@sport/[package-name]' for scoped packages, 2) 'description' field to reflect multi-provider AI CLI capabilities, 3) 'bin' field to change command from 'gemini' to 'sport', 4) 'keywords' array to include 'multi-provider', 'ai-cli', 'openrouter', 'deepseek' etc., 5) 'homepage', 'repository', and 'bugs' URLs if they contain 'gemini' references",
            "status": "done",
            "testStrategy": "Run 'npm install' and 'npm run build' in each package to ensure package.json changes don't break the build. Verify 'npm link' works correctly with new package names"
          },
          {
            "id": 2,
            "title": "Update CLI command with backward compatibility",
            "description": "Change the main CLI command from 'gemini' to 'sport' while maintaining 'gemini' as a deprecated alias for backward compatibility",
            "dependencies": [
              1
            ],
            "details": "In the main CLI entry point (likely in packages/cli/src/index.ts or similar): 1) Update the primary command registration to use 'sport', 2) Add 'gemini' as an alias that shows a deprecation warning before executing the same functionality, 3) Update commander or yargs configuration to reflect the new command name, 4) Add a deprecation notice that appears when using 'gemini' command suggesting users switch to 'sport'",
            "status": "done",
            "testStrategy": "Test both 'sport' and 'gemini' commands work correctly. Verify deprecation warning appears only for 'gemini' command. Test all subcommands work with both aliases"
          },
          {
            "id": 3,
            "title": "Update all README.md files",
            "description": "Replace all references to 'Gemini CLI' with 'sport-cli' in README files throughout the repository",
            "dependencies": [
              1
            ],
            "details": "Update: 1) Main README.md in repository root - change title, badges, installation instructions, usage examples, 2) Package-specific README files in each package directory, 3) Replace 'Google Gemini CLI' with 'sport-cli - Multi-Provider AI CLI', 4) Update installation commands from 'npm install -g gemini-cli' to 'npm install -g sport-cli', 5) Update all usage examples to use 'sport' command instead of 'gemini', 6) Add section highlighting multi-provider support (OpenRouter, DeepSeek, etc.)",
            "status": "pending",
            "testStrategy": "Use grep to ensure no 'Gemini CLI' or 'gemini-cli' references remain in README files. Manually review formatting and ensure all examples are updated"
          },
          {
            "id": 4,
            "title": "Update documentation in docs/ folder",
            "description": "Update all documentation files to reflect the rebranding from Gemini CLI to sport-cli",
            "dependencies": [
              3
            ],
            "details": "Search docs/ folder for all .md, .mdx, or other documentation files and update: 1) Replace 'Gemini CLI' with 'sport-cli' in all text content, 2) Update code examples to use 'sport' command, 3) Update configuration examples to use new paths and environment variables, 4) Update any diagrams or images that contain 'Gemini' branding, 5) Ensure API documentation reflects multi-provider support, 6) Update any provider-specific guides to be provider-agnostic",
            "status": "pending",
            "testStrategy": "Build documentation site (if applicable) and verify all links work. Search for remaining 'gemini' references. Review generated documentation for consistency"
          },
          {
            "id": 5,
            "title": "Update hardcoded strings in source code",
            "description": "Find and replace all hardcoded 'Gemini CLI' references in TypeScript/JavaScript source files",
            "dependencies": [
              2
            ],
            "details": "Search all .ts, .tsx, .js, .jsx files for: 1) String literals containing 'Gemini CLI', 'Google Gemini CLI', or similar, 2) Update CLI welcome messages, help text, error messages to use 'sport-cli', 3) Update any console.log statements, 4) Update error messages to be provider-agnostic, 5) Search for 'gemini' in variable names, function names, and class names - rename to generic terms like 'provider', 'ai', or 'sport', 6) Update any hardcoded URLs or API endpoints that reference Gemini",
            "status": "pending",
            "testStrategy": "Run the CLI and verify all user-facing text is updated. Use automated tests to check error messages. Grep for remaining 'gemini' references in source code"
          },
          {
            "id": 6,
            "title": "Migrate configuration directory with backward compatibility",
            "description": "Change configuration directory from .gemini to .sport while providing automatic migration for existing users",
            "dependencies": [
              5
            ],
            "details": "Implement configuration migration: 1) Update default config directory path from '~/.gemini' to '~/.sport', 2) On startup, check if .gemini exists but .sport doesn't - if so, copy/migrate configuration, 3) Update all file I/O operations to use new path, 4) Create migration function that preserves user settings, API keys, and preferences, 5) Log migration status to inform users, 6) Update config file names if they contain 'gemini' (e.g., gemini-config.json to sport-config.json)",
            "status": "pending",
            "testStrategy": "Test fresh installation creates .sport directory. Test existing .gemini directory is migrated correctly. Verify all settings are preserved after migration"
          },
          {
            "id": 7,
            "title": "Update environment variables with backward compatibility",
            "description": "Change environment variable prefixes from GEMINI_ to SPORT_ while maintaining support for old variables",
            "dependencies": [
              6
            ],
            "details": "Update environment variable handling: 1) Change all GEMINI_ prefixed variables to SPORT_ (e.g., GEMINI_API_KEY to SPORT_API_KEY), 2) Implement fallback logic that checks for old variables if new ones aren't set, 3) Add deprecation warnings when old variables are used, 4) Update .env.example files with new variable names, 5) Document both old and new variables in configuration guide, 6) Consider generic names like SPORT_OPENROUTER_API_KEY for provider-specific keys",
            "status": "pending",
            "testStrategy": "Test both old and new environment variables work. Verify deprecation warnings appear for old variables. Test precedence when both are set"
          },
          {
            "id": 8,
            "title": "Update CLI help text and command descriptions",
            "description": "Update all help text, command descriptions, and usage information to reflect sport-cli branding",
            "dependencies": [
              5,
              7
            ],
            "details": "Update command help text: 1) Main help banner should show 'sport-cli' and describe multi-provider capabilities, 2) Update all command descriptions to be provider-agnostic, 3) Update examples in help text to use 'sport' command, 4) Ensure --help output for all subcommands is updated, 5) Update version command output to show 'sport-cli', 6) Add provider list to help text showing supported providers (OpenRouter, DeepSeek, etc.)",
            "status": "pending",
            "testStrategy": "Run 'sport --help' and all subcommand help options. Verify all text is updated and no 'gemini' references remain. Check help text formatting is correct"
          },
          {
            "id": 9,
            "title": "Update license headers and copyright notices",
            "description": "Update any license headers, copyright notices, and legal text to reflect the new sport-cli branding if needed",
            "dependencies": [
              8
            ],
            "details": "Review and update: 1) File headers in source code files that mention 'Gemini CLI', 2) LICENSE file if it contains specific product references, 3) Copyright notices in documentation, 4) Any third-party attribution files, 5) Update author/maintainer information if needed, 6) Ensure open source compliance for multi-provider support",
            "status": "pending",
            "testStrategy": "Grep for copyright notices and license headers. Verify LICENSE file is appropriate for multi-provider CLI. Check all legal requirements are met"
          },
          {
            "id": 10,
            "title": "Create comprehensive migration guide",
            "description": "Create a migration guide for users upgrading from the Gemini-specific version to sport-cli",
            "dependencies": [
              1,
              2,
              3,
              4,
              5,
              6,
              7,
              8,
              9
            ],
            "details": "Create MIGRATION.md file containing: 1) Overview of changes from gemini-cli to sport-cli, 2) Step-by-step upgrade instructions, 3) Command mapping table (gemini -> sport), 4) Environment variable mapping (GEMINI_* -> SPORT_*), 5) Configuration directory migration (.gemini -> .sport), 6) Breaking changes and how to address them, 7) Benefits of multi-provider support, 8) FAQ section for common migration issues, 9) Rollback instructions if needed, 10) Timeline for deprecation of old commands/variables",
            "status": "pending",
            "testStrategy": "Follow the migration guide on a system with old gemini-cli installed. Verify all steps work correctly. Have team members review for clarity and completeness"
          }
        ]
      },
      {
        "id": 13,
        "title": "Fix CLI Test Failures for Slash Commands and Compression",
        "description": "Resolve test failures in the CLI package specifically related to slash command processing and compression functionality, excluding any failures caused by branding changes.",
        "details": "1. **Identify Failing Tests**: Run the test suite for the CLI package and identify all failing tests related to slash command processing and compression. Filter out any failures that are due to branding changes (Task #12).\n\n2. **Slash Command Processing Fixes**:\n   - Review test files in `/packages/cli/src/commands/` and `/packages/cli/src/ui/hooks/` directories\n   - Common issues to check:\n     - Command parsing logic expecting specific formats or delimiters\n     - Command handler registration and routing\n     - Async command execution and promise handling\n     - Input validation and error handling for malformed commands\n   - Update command processors to handle edge cases like empty commands, special characters, or concurrent command execution\n\n3. **Compression-Related Fixes**:\n   - Locate compression-related tests (likely in utilities or middleware)\n   - Common compression issues:\n     - Incorrect compression/decompression of command outputs\n     - Buffer handling for large outputs\n     - Encoding issues with compressed data\n     - Stream processing for real-time compression\n   - Ensure compression works correctly with various data types (text, JSON, binary)\n\n4. **Test Environment Setup**:\n   - Verify test environment configuration matches production settings\n   - Check for missing mock implementations or stubs\n   - Ensure test fixtures are up-to-date with current data structures\n   - Review test timeouts for async operations\n\n5. **Code Updates**:\n   - Fix implementation code based on test expectations\n   - Update deprecated API calls or library methods\n   - Ensure proper error handling and edge case coverage\n   - Add missing type definitions if using TypeScript\n\n6. **Test Refactoring** (if needed):\n   - Update outdated test assertions\n   - Fix flaky tests by adding proper wait conditions\n   - Improve test isolation to prevent cross-test contamination\n   - Add missing test cases discovered during debugging",
        "testStrategy": "1. **Baseline Test Run**: Execute `npm test` or `yarn test` in the CLI package directory and document all failing tests related to slash commands and compression. Create a checklist of failures to track progress.\n\n2. **Isolated Test Execution**: Run failing tests individually using test runners' focused mode (e.g., `test.only()` or `--testNamePattern`) to isolate issues and reduce noise from other tests.\n\n3. **Debug Mode Testing**: Run tests with verbose logging or debug flags to capture detailed error messages and stack traces. Use debugger breakpoints in both test and implementation code.\n\n4. **Regression Testing**: After fixing each test, run the entire test suite to ensure no new failures are introduced. Pay special attention to related functionality that might be affected.\n\n5. **Edge Case Verification**:\n   - Test slash commands with special characters: `/test!@#$`, `/cmd with spaces`\n   - Test compression with various data sizes: empty, small (< 1KB), medium (1MB), large (> 10MB)\n   - Test error scenarios: invalid commands, compression failures, timeout conditions\n\n6. **Integration Testing**: Verify that fixed functionality works correctly in the actual CLI environment, not just in unit tests. Manually test key slash commands and compression features.\n\n7. **Performance Testing**: Ensure that fixes don't introduce performance regressions, especially for compression operations on large datasets.\n\n8. **CI/CD Verification**: Confirm all tests pass in the continuous integration environment, which may have different configurations than local development.",
        "status": "pending",
        "dependencies": [
          9
        ],
        "priority": "medium",
        "subtasks": []
      },
      {
        "id": 14,
        "title": "Fix OpenRouter API Error for Invalid Model ID",
        "description": "Resolve the error where 'gemini-2.5-pro' is not recognized as a valid model ID by OpenRouter, and update the default model configuration to use a valid OpenRouter-supported model.",
        "details": "1. **Identify the Issue Location**: Search the codebase for hardcoded references to 'gemini-2.5-pro' model ID, particularly in:\n   - Default configuration files (config.json, .env.example, etc.)\n   - Model selection logic in provider-specific implementations\n   - CLI initialization code where default models are set\n   - Any fallback model configurations\n\n2. **Verify Valid OpenRouter Models**: \n   - Check OpenRouter's API documentation or use their models endpoint to get a list of valid model IDs\n   - Common valid OpenRouter models include: 'openai/gpt-4', 'anthropic/claude-2', 'meta-llama/llama-2-70b-chat', 'google/palm-2-chat-bison', etc.\n   - Choose an appropriate default model that balances capability and cost\n\n3. **Update Default Configuration**:\n   - Replace 'gemini-2.5-pro' with a valid OpenRouter model ID (e.g., 'openai/gpt-3.5-turbo' for a cost-effective default)\n   - Ensure the replacement model is consistently available and well-supported\n   - Update any configuration schemas or TypeScript interfaces that define model IDs\n\n4. **Implement Model Validation**:\n   - Add a validation function that checks if a model ID is valid for the selected provider\n   - Consider caching the list of valid models from OpenRouter to avoid repeated API calls\n   - Provide helpful error messages when an invalid model is specified\n\n5. **Update Documentation**:\n   - Update any documentation that references the old default model\n   - Add notes about OpenRouter's model naming convention (provider/model-name format)\n   - Include examples of valid OpenRouter model IDs in configuration documentation\n\n6. **Consider Provider-Specific Defaults**:\n   - Implement a mapping of provider-specific default models\n   - For OpenRouter, use a widely available model like 'openai/gpt-3.5-turbo'\n   - For direct provider connections, use their native model IDs",
        "testStrategy": "1. **Unit Tests for Model Validation**:\n   - Create tests that verify the model validation function correctly identifies valid and invalid OpenRouter model IDs\n   - Test that appropriate error messages are returned for invalid models\n   - Mock the OpenRouter API response for model listing to ensure consistent testing\n\n2. **Integration Tests with OpenRouter**:\n   - Test making actual API calls with the new default model to ensure it works correctly\n   - Verify that the model switch from 'gemini-2.5-pro' to the new default doesn't break existing functionality\n   - Test error handling when an invalid model is specified in configuration\n\n3. **Configuration Tests**:\n   - Verify that the default configuration loads the correct OpenRouter model\n   - Test that environment variable overrides work correctly with the new model ID\n   - Ensure backward compatibility for users who might have custom configurations\n\n4. **CLI Flow Testing**:\n   - Manually test the CLI with OpenRouter provider to ensure no 'invalid model' errors occur\n   - Test model selection flow to verify users can see and select valid OpenRouter models\n   - Verify that helpful error messages appear if users try to use incompatible model IDs\n\n5. **Regression Testing**:\n   - Run the full test suite to ensure the model change doesn't break other functionality\n   - Pay special attention to any tests that might have hardcoded the old model ID\n   - Test with multiple providers to ensure the fix doesn't affect non-OpenRouter providers",
        "status": "pending",
        "dependencies": [
          1,
          6
        ],
        "priority": "high",
        "subtasks": []
      },
      {
        "id": 15,
        "title": "Fix Terminal Bold Formatting for Model Names",
        "description": "Implement proper ANSI escape code handling to render bold text correctly in terminal output instead of showing raw markdown asterisks",
        "details": "Research indicates the issue stems from MessageType.GEMINI not properly converting markdown bold syntax to ANSI codes. Implement a markdown-to-ANSI converter using chalk@5.3.0 or ansi-styles@6.2.1. Create a utility function that detects **text** patterns and replaces with \\x1b[1mtext\\x1b[0m sequences. For cross-platform compatibility, use supports-color@9.4.0 to detect terminal capabilities. Implementation: \n\n```typescript\nimport chalk from 'chalk';\nimport supportsColor from 'supports-color';\n\nfunction formatBoldText(text: string): string {\n  if (!supportsColor.stdout) return text.replace(/\\*\\*(.*?)\\*\\*/g, '$1');\n  return text.replace(/\\*\\*(.*?)\\*\\*/g, (_, p1) => chalk.bold(p1));\n}\n```\n\nIntegrate this into the MessageType.GEMINI rendering pipeline to ensure all table content processes through this formatter.",
        "testStrategy": "Create unit tests with various markdown bold patterns including edge cases like nested asterisks, incomplete patterns, and mixed content. Test on different terminal emulators (Windows Terminal, iTerm2, standard Linux terminals) to ensure cross-platform compatibility. Verify that when terminal doesn't support colors, bold markers are cleanly removed.",
        "priority": "high",
        "dependencies": [],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 16,
        "title": "Implement Dynamic Column Width Calculation for Table Display",
        "description": "Create an intelligent column width system that prevents truncation of headers like 'Context' and adjusts to terminal width constraints",
        "details": "Implement a dynamic column width calculator using terminal-size@3.0.0 to detect available width. Research shows cli-table3@0.6.3 provides excellent table rendering with automatic width calculation. Implementation approach:\n\n```typescript\nimport Table from 'cli-table3';\nimport terminalSize from 'terminal-size';\n\nfunction createModelTable(models: Model[]): string {\n  const { columns } = terminalSize();\n  const table = new Table({\n    head: ['Model', 'Context', 'Cost', 'Best For'],\n    colWidths: [\n      Math.floor(columns * 0.35), // 35% for model name\n      Math.floor(columns * 0.15), // 15% for context\n      Math.floor(columns * 0.20), // 20% for cost\n      Math.floor(columns * 0.30)  // 30% for best for\n    ],\n    wordWrap: true,\n    wrapOnWordBoundary: true\n  });\n  \n  // Add responsive breakpoints\n  if (columns < 80) {\n    // Compact mode for narrow terminals\n    table.options.colWidths = [25, 10, 15, 30];\n  }\n  \n  return table.toString();\n}\n```\n\nEnsure minimum column widths to prevent 'Con...' truncation. Add ellipsis handling for extremely long content while preserving critical information.",
        "testStrategy": "Test with various terminal widths from 60 to 200 columns. Verify that 'Context' header never truncates to 'Con...'. Create test cases with extremely long model names and descriptions to ensure graceful handling. Mock terminal-size to test different viewport scenarios. Validate table alignment remains consistent across all widths.",
        "priority": "high",
        "dependencies": [
          15
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 17,
        "title": "Build Intelligent Model Deduplication and Curation System",
        "description": "Implement logic to select exactly one best Dolphin model plus one flagship model from each major provider, eliminating duplicate recommendations",
        "details": "Create a sophisticated model curation system that groups models by provider and selects the best from each category. Use a scoring algorithm based on context length, cost efficiency, and capability markers. Implementation:\n\n```typescript\ninterface ModelScore {\n  model: Model;\n  score: number;\n  category: 'dolphin' | 'openai' | 'anthropic' | 'google' | 'xai' | 'other';\n}\n\nfunction curateModels(allModels: Model[]): Model[] {\n  const categorized = new Map<string, ModelScore[]>();\n  \n  allModels.forEach(model => {\n    const category = detectCategory(model.id);\n    const score = calculateScore(model);\n    \n    if (!categorized.has(category)) {\n      categorized.set(category, []);\n    }\n    categorized.get(category)!.push({ model, score, category });\n  });\n  \n  // Select best from each category\n  const selected: Model[] = [];\n  \n  // Prioritize Dolphin\n  const dolphinModels = categorized.get('dolphin') || [];\n  if (dolphinModels.length > 0) {\n    const best = dolphinModels.sort((a, b) => b.score - a.score)[0];\n    selected.push(best.model);\n  }\n  \n  // Add one from each major provider\n  ['openai', 'anthropic', 'google', 'xai'].forEach(provider => {\n    const models = categorized.get(provider) || [];\n    if (models.length > 0) {\n      const best = models.sort((a, b) => b.score - a.score)[0];\n      selected.push(best.model);\n    }\n  });\n  \n  return selected;\n}\n\nfunction calculateScore(model: Model): number {\n  let score = 0;\n  \n  // Context length scoring (logarithmic scale)\n  score += Math.log10(model.contextLength) * 10;\n  \n  // Cost efficiency (inverse relationship)\n  const avgCost = (model.inputCost + model.outputCost) / 2;\n  score += 100 / (avgCost + 1);\n  \n  // Boost for newer models\n  if (model.id.includes('4') || model.id.includes('3.5')) score += 20;\n  if (model.id.includes('turbo')) score += 10;\n  \n  return score;\n}\n```",
        "testStrategy": "Create comprehensive test suite with mock model data including multiple Dolphin variants, various GPT models, Claude versions, and Gemini models. Verify exactly one model per provider is selected. Test edge cases like missing providers, single model scenarios, and identical scoring situations. Validate that the highest-scoring Dolphin model is always selected when available.",
        "priority": "high",
        "dependencies": [],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 18,
        "title": "Implement Cost Display Formatting and Explanation System",
        "description": "Create a clear, consistent cost display format with complete explanation text that doesn't get cut off",
        "details": "Design a cost formatting system that displays prices clearly and includes a comprehensive explanation. Use Intl.NumberFormat for locale-aware formatting and implement smart unit selection (per 1K or 1M tokens based on magnitude). Implementation:\n\n```typescript\nfunction formatCost(inputCost: number, outputCost: number): string {\n  const formatter = new Intl.NumberFormat('en-US', {\n    style: 'currency',\n    currency: 'USD',\n    minimumFractionDigits: 2,\n    maximumFractionDigits: 4\n  });\n  \n  // Determine optimal unit (1K vs 1M tokens)\n  const avgCost = (inputCost + outputCost) / 2;\n  const useMillions = avgCost < 0.01;\n  \n  if (useMillions) {\n    const inCost = formatter.format(inputCost * 1000);\n    const outCost = formatter.format(outputCost * 1000);\n    return `${inCost}/${outCost} per 1M`;\n  } else {\n    const inCost = formatter.format(inputCost);\n    const outCost = formatter.format(outputCost);\n    return `${inCost}/${outCost} per 1K`;\n  }\n}\n\nfunction generateCostExplanation(): string {\n  return `\n💰 Cost Format: $input/$output per 1K tokens\n   • Input cost: Price for text you send to the model\n   • Output cost: Price for text generated by the model\n   • 1K tokens ≈ 750 words\n   • Prices from OpenRouter as of ${new Date().toLocaleDateString()}\n`.trim();\n}\n```\n\nEnsure explanation text appears below the table with proper spacing and doesn't get truncated by terminal width constraints.",
        "testStrategy": "Test cost formatting with various price ranges from $0.0001 to $10.00. Verify automatic unit switching between 1K and 1M tokens. Test explanation text rendering at different terminal widths to ensure no truncation. Validate currency formatting works correctly with different locales. Create snapshot tests for consistent output format.",
        "priority": "medium",
        "dependencies": [
          16
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 19,
        "title": "Create Enhanced Model Metadata and 'Best For' Descriptions",
        "description": "Implement a system to generate meaningful, specific 'Best For' descriptions for each model based on their characteristics and capabilities",
        "details": "Build a metadata enrichment system that analyzes model capabilities and generates targeted use-case descriptions. Use a combination of model ID parsing, capability detection, and predefined templates. Implementation:\n\n```typescript\ninterface ModelMetadata {\n  strengths: string[];\n  bestFor: string;\n  category: string;\n}\n\nfunction generateModelMetadata(model: Model): ModelMetadata {\n  const metadata: ModelMetadata = {\n    strengths: [],\n    bestFor: '',\n    category: detectModelCategory(model.id)\n  };\n  \n  // Analyze context length\n  if (model.contextLength >= 128000) {\n    metadata.strengths.push('long documents');\n  } else if (model.contextLength >= 32000) {\n    metadata.strengths.push('extended conversations');\n  }\n  \n  // Analyze cost efficiency\n  const avgCost = (model.inputCost + model.outputCost) / 2;\n  if (avgCost < 0.001) {\n    metadata.strengths.push('high-volume tasks');\n  }\n  \n  // Model-specific capabilities\n  const templates: Record<string, string> = {\n    'dolphin': 'Uncensored coding & technical tasks',\n    'gpt-4': 'Complex reasoning & analysis',\n    'claude-3': 'Long-form content & code review',\n    'gemini': 'Multimodal tasks & large contexts',\n    'grok': 'Real-time info & casual coding'\n  };\n  \n  // Generate 'Best For' description\n  const baseTemplate = templates[metadata.category] || 'General purpose';\n  const strengths = metadata.strengths.slice(0, 2).join(', ');\n  \n  metadata.bestFor = strengths \n    ? `${baseTemplate}, ${strengths}`\n    : baseTemplate;\n    \n  // Ensure description fits in column width\n  if (metadata.bestFor.length > 40) {\n    metadata.bestFor = metadata.bestFor.substring(0, 37) + '...';\n  }\n  \n  return metadata;\n}\n```\n\nIntegrate with the model display system to show contextual, valuable information for each model.",
        "testStrategy": "Create test cases for each model category with various context lengths and cost profiles. Verify descriptions are meaningful and differentiated. Test truncation logic for long descriptions. Validate that each major model gets an appropriate, specific description. Use snapshot testing to ensure consistency in description generation.",
        "priority": "medium",
        "dependencies": [
          17
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 20,
        "title": "Add Usage Examples and Model List Navigation",
        "description": "Implement helpful usage examples and links to comprehensive model lists, enhancing the command's utility and discoverability",
        "details": "Create a comprehensive help section that appears after the model table, providing usage examples and navigation options. Use terminal-link@3.0.0 for clickable URLs in supported terminals. Implementation:\n\n```typescript\nimport terminalLink from 'terminal-link';\n\nfunction generateUsageSection(): string {\n  const sections = [];\n  \n  // Usage examples\n  sections.push('📝 Usage Examples:');\n  sections.push('  sport-cli chat --model dolphin-mixtral     # Start chat with Dolphin');\n  sections.push('  sport-cli ask \"Fix this bug\" --model gpt-4  # One-shot query');\n  sections.push('  sport-cli config set default-model claude-3  # Set default');\n  sections.push('');\n  \n  // Links section\n  sections.push('🔗 More Options:');\n  \n  const fullListUrl = 'https://openrouter.ai/models';\n  const docsUrl = 'https://sport-cli.dev/models';\n  \n  if (terminalLink.isSupported) {\n    sections.push(`  • ${terminalLink('View all 200+ models', fullListUrl)}`);\n    sections.push(`  • ${terminalLink('Model comparison guide', docsUrl)}`);\n  } else {\n    sections.push(`  • View all 200+ models: ${fullListUrl}`);\n    sections.push(`  • Model comparison guide: ${docsUrl}`);\n  }\n  \n  sections.push('');\n  sections.push('💡 Tip: Use \"sport-cli models --all\" to see extended list');\n  \n  return sections.join('\\n');\n}\n\n// Add command flag handling\ninterface ModelCommandOptions {\n  all?: boolean;\n  category?: string;\n  maxCost?: number;\n  minContext?: number;\n}\n\nfunction handleModelCommand(options: ModelCommandOptions): void {\n  if (options.all) {\n    displayAllModels();\n  } else {\n    displayCuratedModels();\n    console.log(generateUsageSection());\n  }\n}\n```\n\nEnsure the usage section is visually distinct from the table and provides immediate value to users.",
        "testStrategy": "Test terminal link support detection and fallback behavior. Verify usage examples are accurate and executable. Test with various terminal emulators to ensure links work where supported. Validate that the help section doesn't interfere with table display. Create integration tests that verify the complete command output including table, explanation, and usage sections.",
        "priority": "low",
        "dependencies": [
          15,
          16,
          17,
          18,
          19
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 21,
        "title": "Create Task Analysis System for AI Model Selection",
        "description": "Implement a system that evaluates each task's complexity, requirements, and recommends the optimal AI model for implementation based on factors like code complexity, reasoning requirements, context needs, and cost-effectiveness",
        "details": "Build a comprehensive task analysis system that intelligently recommends AI models for specific tasks. The system should analyze multiple dimensions of task complexity and match them to model capabilities. Implementation approach:\n\n```typescript\ninterface TaskAnalysis {\n  taskId: number;\n  complexity: {\n    codeComplexity: 'low' | 'medium' | 'high';\n    reasoningDepth: 'shallow' | 'moderate' | 'deep';\n    contextRequirement: number; // estimated tokens needed\n    creativityNeeded: boolean;\n    precisionRequired: boolean;\n  };\n  recommendedModel: {\n    primary: string;\n    alternative: string;\n    rationale: string;\n  };\n  costEstimate: {\n    estimatedTokens: number;\n    costWithPrimary: number;\n    costWithAlternative: number;\n  };\n}\n\nclass TaskAnalyzer {\n  private modelCapabilities: Map<string, ModelProfile>;\n  \n  analyzeTask(task: Task): TaskAnalysis {\n    const complexity = this.assessComplexity(task);\n    const contextNeeds = this.estimateContextRequirement(task);\n    const modelRecommendation = this.recommendModel(complexity, contextNeeds);\n    \n    return {\n      taskId: task.id,\n      complexity,\n      recommendedModel: modelRecommendation,\n      costEstimate: this.calculateCosts(task, modelRecommendation)\n    };\n  }\n  \n  private assessComplexity(task: Task): TaskComplexity {\n    // Analyze task description for keywords indicating complexity\n    const complexityIndicators = {\n      high: ['algorithm', 'optimization', 'architecture', 'system design', 'complex'],\n      medium: ['implement', 'integrate', 'refactor', 'enhance'],\n      low: ['update', 'fix', 'add', 'simple', 'basic']\n    };\n    \n    // Check for reasoning depth requirements\n    const reasoningIndicators = {\n      deep: ['analyze', 'evaluate', 'design', 'architect', 'strategy'],\n      moderate: ['implement', 'build', 'create', 'develop'],\n      shallow: ['update', 'modify', 'fix', 'adjust']\n    };\n    \n    // Scan task details for code blocks and technical depth\n    const codeBlockCount = (task.details.match(/```/g) || []).length / 2;\n    const hasArchitecturalDecisions = /interface|class|architecture|design pattern/i.test(task.details);\n    \n    return {\n      codeComplexity: this.categorizeCodeComplexity(codeBlockCount, hasArchitecturalDecisions),\n      reasoningDepth: this.categorizeReasoningDepth(task.description + task.details),\n      contextRequirement: this.estimateTokens(task),\n      creativityNeeded: /design|create|innovative|novel/i.test(task.description),\n      precisionRequired: /critical|exact|precise|accurate/i.test(task.description)\n    };\n  }\n  \n  private recommendModel(complexity: TaskComplexity, contextNeeds: number): ModelRecommendation {\n    // Model selection logic based on task requirements\n    if (complexity.reasoningDepth === 'deep' || complexity.codeComplexity === 'high') {\n      return {\n        primary: 'o3-mini', // or 'claude-3-opus' for deep reasoning\n        alternative: 'claude-3-sonnet',\n        rationale: 'Complex task requiring deep reasoning and sophisticated code generation'\n      };\n    }\n    \n    if (complexity.reasoningDepth === 'moderate' && contextNeeds < 50000) {\n      return {\n        primary: 'claude-3-sonnet',\n        alternative: 'gpt-4-turbo',\n        rationale: 'Balanced task requiring good reasoning with moderate context'\n      };\n    }\n    \n    if (complexity.codeComplexity === 'low' && !complexity.creativityNeeded) {\n      return {\n        primary: 'gemini-1.5-flash',\n        alternative: 'claude-3-haiku',\n        rationale: 'Simple task suitable for fast, efficient models'\n      };\n    }\n    \n    // Context-heavy tasks\n    if (contextNeeds > 100000) {\n      return {\n        primary: 'gemini-1.5-pro', // 2M context\n        alternative: 'claude-3-sonnet',\n        rationale: 'Task requires extensive context window'\n      };\n    }\n    \n    // Default balanced recommendation\n    return {\n      primary: 'claude-3-sonnet',\n      alternative: 'gpt-4-turbo',\n      rationale: 'Standard development task with balanced requirements'\n    };\n  }\n}\n\n// Integration with existing model system\nfunction integrateWithModelCommand(analyzer: TaskAnalyzer): void {\n  // Add task analysis results to model selection UI\n  const analysisResults = analyzer.analyzeAllTasks();\n  \n  // Display recommendations in model list\n  console.log('📊 Task-Optimized Model Recommendations:');\n  analysisResults.forEach(analysis => {\n    console.log(`Task #${analysis.taskId}: ${analysis.recommendedModel.primary}`);\n    console.log(`  Rationale: ${analysis.recommendedModel.rationale}`);\n    console.log(`  Est. Cost: ${formatCurrency(analysis.costEstimate.costWithPrimary)}`);\n  });\n}\n```\n\nThe system should also include a CLI interface for analyzing specific tasks:\n\n```bash\nsport-cli analyze-task 21  # Analyze specific task\nsport-cli analyze-all      # Analyze all tasks and generate report\nsport-cli recommend --task \"implement caching system\"  # Get model recommendation for description\n```",
        "testStrategy": "Create comprehensive test suite covering all aspects of the task analysis system:\n\n1. **Complexity Assessment Tests**: Create test cases with various task descriptions representing different complexity levels. Verify that code complexity detection correctly identifies simple updates vs. architectural changes. Test reasoning depth categorization with tasks ranging from simple fixes to system design.\n\n2. **Context Estimation Tests**: Test token estimation accuracy by comparing with actual tokenizer counts for various task sizes. Create edge cases with very large tasks (>100k tokens) and minimal tasks (<1k tokens). Verify context requirement calculations include dependencies and related code.\n\n3. **Model Recommendation Tests**: Test recommendation logic with tasks of varying complexity profiles. Verify that high-complexity tasks recommend powerful models (O3/Opus), medium tasks recommend balanced models (Sonnet), and simple tasks recommend fast models (Flash/Haiku). Test edge cases where multiple factors conflict (e.g., simple logic but huge context).\n\n4. **Cost Calculation Tests**: Verify cost estimates are accurate based on model pricing and estimated token usage. Test that alternative model costs are calculated correctly. Create scenarios with different token volumes to ensure pricing tiers are handled properly.\n\n5. **Integration Tests**: Test CLI commands with mock task data. Verify analyze-task command produces correct output format. Test analyze-all generates comprehensive reports. Validate that recommendations integrate properly with existing model selection system.\n\n6. **Performance Tests**: Ensure analysis of large task sets (100+ tasks) completes in reasonable time. Test memory usage with extensive task histories. Verify caching mechanisms work for repeated analyses.",
        "status": "pending",
        "dependencies": [
          17,
          19
        ],
        "priority": "high",
        "subtasks": []
      }
    ],
    "metadata": {
      "created": "2025-07-07T18:56:28.419Z",
      "updated": "2025-07-19T18:59:45.501Z",
      "description": "Tasks for master context"
    }
  }
}