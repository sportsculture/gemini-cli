{
  "master": {
    "tasks": [
      {
        "id": 1,
        "title": "Enhance AuthDialog for OpenRouter API Key Prompt",
        "description": "Modify AuthDialog.tsx to prompt for OpenRouter API key when selected, ensuring secure handling and environment/config storage.",
        "details": "Update /packages/cli/src/ui/components/AuthDialog.tsx to include a prompt for OpenRouter API key input. Use secure input handling (e.g., masking). Store the API key in environment variables or a secure config file (e.g., dotenv for Node.js, or a custom encrypted config). Check for existing API keys in environment variables and skip prompt if present. Use react-hook-form for form state management (v7+).",
        "testStrategy": "Test that the prompt appears only for OpenRouter selection. Verify API key is stored securely and not logged. Test environment variable override. Manual and automated UI tests.",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 2,
        "title": "Implement API Key Validation and Error Handling",
        "description": "Add logic to validate OpenRouter API keys and provide user feedback for invalid or missing keys.",
        "details": "In /packages/cli/src/ui/hooks/useAuthCommand.ts, implement API key validation by making a test request to OpenRouter (e.g., GET /api/keys endpoint). Use axios (v1.6+) for HTTP requests. Display clear error messages for invalid or missing keys. Handle rate limits and API errors gracefully. Store validated API key securely.",
        "testStrategy": "Test with valid, invalid, and missing API keys. Verify error messages and graceful handling. Automated integration tests for API key validation.",
        "priority": "high",
        "dependencies": [
          1
        ],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 3,
        "title": "Add Model Listing and Selection UI",
        "description": "Implement UI to list available OpenRouter models and allow user selection, consistent with existing auth patterns.",
        "details": "Extend AuthDialog.tsx or create a new ModelSelection component to list models fetched from OpenRouter (GET /api/models). Use react-select (v5+) for dropdown. Save selected model in state and config. Ensure UI matches existing sprtscltr patterns. Use react-query (v4+) for data fetching and caching.",
        "testStrategy": "Test model listing and selection. Verify UI consistency. Manual and automated UI tests.",
        "priority": "medium",
        "dependencies": [
          2
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Fetch available models from OpenRouter API using react-query",
            "description": "Implement data fetching logic to retrieve available models from the OpenRouter API, utilizing react-query for caching and state management. Ensure the query key includes relevant parameters such as baseUrl for proper cache invalidation.",
            "dependencies": [],
            "details": "Use the useOpenRouterModelProviders hook or similar, passing the custom baseUrl from user configuration. Ensure the queryKey includes baseUrl as part of its key for correct caching and refetching when the endpoint changes.\n<info added on 2025-07-08T05:29:52.661Z>\nPlan update:\n- Implement a fetchModels method within OpenRouterContentGenerator to retrieve available models from the configured endpoint.\n- Create a useOpenRouterModels hook that uses fetchModels to fetch and locally cache the model list, ensuring efficient reuse and updates when the baseUrl changes.\n- Refactor ModelSelector to consume the models provided by useOpenRouterModels instead of relying on hardcoded values.\n- Add logic to ModelSelector to display loading indicators while fetching and handle error states gracefully if model retrieval fails.\n</info added on 2025-07-08T05:29:52.661Z>\n<info added on 2025-07-08T05:31:25.764Z>\nCreated fetchModels method in OpenRouterContentGenerator to retrieve models from the /api/v1/models endpoint using the configured baseUrl. Developed useOpenRouterModels React hook with a 5-minute cache TTL and integrated error handling. Updated ModelSelector to consume the dynamic model list from useOpenRouterModels, replacing the previous hardcoded values. Implemented loading indicators and error state handling in ModelSelector to improve user experience during model retrieval.\n</info added on 2025-07-08T05:31:25.764Z>",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Implement or extend ModelSelection component",
            "description": "Create a new ModelSelection component or extend the existing AuthDialog.tsx to include model selection functionality, integrating the data fetched in the previous step.",
            "dependencies": [
              1
            ],
            "details": "Ensure the component receives the list of models as props or via hook, and is structured to allow for easy integration of a dropdown UI.\n<info added on 2025-07-08T05:34:52.821Z>\nIntegrated the ModelSelector component into the main App.tsx rendering flow, utilizing the useModelSelector hook for managing model selection state. Implemented a /model slash command that triggers the model selector UI, which is only accessible when authenticated with OpenRouter. Ensured all components are properly connected with state management so that the model selector appears dynamically upon typing the /model command.\n</info added on 2025-07-08T05:34:52.821Z>",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Integrate react-select (v5+) for dropdown UI",
            "description": "Replace or enhance the model selection UI with react-select (v5+) to provide a user-friendly dropdown for model selection.",
            "dependencies": [
              2
            ],
            "details": "Configure react-select with the fetched model data, ensuring accessibility and usability. Style the dropdown to match existing UI patterns.\n<info added on 2025-07-08T05:38:46.371Z>\nRadioButtonSelect will be retained for model selection to maintain UI consistency with other dialogs (AuthDialog, ThemeDialog, EditorDialog). The ModelSelector component already provides a dropdown interface with loading, error, and keyboard accessibility features. Introducing react-select is unnecessary and would disrupt the established UI pattern while adding extra dependencies.\n</info added on 2025-07-08T05:38:46.371Z>",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Persist selected model in state and config",
            "description": "Ensure that the user's selected model is persisted both in component state and in the application's configuration, so it remains consistent across sessions and reloads.",
            "dependencies": [
              3
            ],
            "details": "Update state management logic to store the selected model, and synchronize with user configuration or context as appropriate.\n<info added on 2025-07-08T05:39:55.856Z>\nThe selected model is currently persisted for the session using config.setModel() within handleModelSelect, aligning with the established pattern where model selection is determined by environment variables (OPENROUTER_MODEL) or defaults. Persisting the selection across sessions would require writing to settings.json, but this is not implemented as it would diverge from the current approach. No changes to cross-session persistence are needed unless the requirements change.\n</info added on 2025-07-08T05:39:55.856Z>",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Ensure UI consistency and write UI tests",
            "description": "Review the implementation for consistency with sprtscltr UI patterns and write both manual and automated UI tests to verify correct behavior and appearance.",
            "dependencies": [
              4
            ],
            "details": "Perform visual and functional checks, update styles as needed, and implement tests using the project's preferred testing framework.\n<info added on 2025-07-08T05:41:35.873Z>\nUI consistency has been ensured by standardizing on the RadioButtonSelect component, mirroring patterns from AuthDialog, ThemeDialog, and EditorDialog, and implementing loading spinners, error states, and keyboard navigation (Enter to select, Escape to cancel). Styling uses Colors constants for visual alignment. Tests have been written for ModelSelector, useOpenRouterModels, and useModelSelector, following project conventions, though dependency issues currently prevent execution.\n</info added on 2025-07-08T05:41:35.873Z>",
            "status": "done",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 4,
        "title": "Enable Model Switching and Persistence",
        "description": "Model switching is already implemented: users can switch models mid-session using the /model command, and the model is persisted for the current session via config.setModel(). The UI displays a model change notification. The implementation follows the existing pattern where the model is determined by environment variables. Cross-session persistence (e.g., saving to settings.json) is not implemented, as this would deviate from the current architecture where auth-related settings come from environment variables.",
        "status": "done",
        "dependencies": [
          3
        ],
        "priority": "medium",
        "details": "No further changes are required for mid-session model switching or session persistence. Cross-session persistence (saving model preference to settings.json) is not implemented to maintain consistency with the current architecture, which relies on environment variables for auth and model configuration. Document this architectural decision and ensure user documentation reflects the current behavior and limitations.",
        "testStrategy": "Verify that users can switch models mid-session using the /model command and that the selected model persists for the duration of the session. Confirm that the UI displays a notification when the model changes. Automated integration tests should cover these behaviors. No tests are required for cross-session persistence, as it is not supported.",
        "subtasks": [
          {
            "id": 1,
            "title": "Document current model switching and persistence behavior",
            "description": "Update user and developer documentation to clarify that model switching is supported mid-session and persists for the session, but cross-session persistence is not implemented due to architectural constraints.",
            "status": "done",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Add automated integration tests for model switching and session persistence",
            "description": "Ensure tests cover switching models with /model command, session persistence, and UI notifications. No tests for cross-session persistence are needed.",
            "status": "done",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 5,
        "title": "Display Current Model in UI",
        "description": "Show the currently selected OpenRouter model in the status bar or header.",
        "details": "Update /packages/cli/src/ui/App.tsx to display the current model. Use context or state management (e.g., Zustand v4+ or React Context) to share model state across components. Ensure display updates on model switch.",
        "testStrategy": "Test that current model is displayed and updates on switch. Manual and automated UI tests.",
        "priority": "medium",
        "dependencies": [
          4
        ],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 6,
        "title": "Ensure Backward Compatibility and Provider Consistency",
        "description": "Maintain compatibility with existing auth methods and ensure custom API providers follow the same pattern.",
        "details": "Review and update all auth and model selection flows to ensure backward compatibility. Apply the same model switching pattern to other providers if supported. Use TypeScript for type safety. Document changes and update configuration schema.",
        "testStrategy": "Test all auth methods and providers. Verify backward compatibility and consistent behavior. Automated integration and regression tests.",
        "priority": "medium",
        "dependencies": [
          5
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 7,
        "title": "Fix OpenRouter/DeepSeek Streaming Output Issue",
        "description": "Resolve the issue where the response is being built character by character, causing repeated text in the streaming output from OpenRouter/DeepSeek API.",
        "details": "To address this issue, implement the following steps: \n1. **Review API Request Structure**: Ensure that the API request to OpenRouter/DeepSeek is correctly formatted to handle streaming responses. This may involve setting the `stream` parameter to `true` in the API call, as shown in the DeepSeek API documentation[1]. \n2. **Buffering and Response Handling**: Implement a buffering mechanism to collect the streaming response in chunks rather than processing it character by character. This can be achieved using a library like `async-iterator` in Node.js or similar constructs in other languages. \n3. **Error Handling and Logging**: Enhance error handling to detect and log any issues that might cause repeated text, such as network errors or API rate limits. Use logging libraries to track these events for debugging purposes. \n4. **Testing with Different Models**: Test the streaming functionality with various models available through OpenRouter to ensure the fix is model-agnostic. \n5. **Code Refactoring**: Refactor the code to improve readability and maintainability, ensuring that the streaming logic is modular and easy to update. \n\nExample code for handling streaming responses in Node.js might look like this: \n```javascript\nimport { createReadStream } from 'fs';\nimport axios from 'axios';\n\nconst apiStream = async () => {\n  const response = await axios.get('https://api.deepseek.com/chat/completions', {\n    params: { stream: true },\n    responseType: 'stream'\n  });\n\n  const chunks = [];\n  response.data.on('data', chunk => chunks.push(chunk));\n  response.data.on('end', () => {\n    const fullResponse = Buffer.concat(chunks).toString();\n    console.log(fullResponse);\n  });\n};\n```\n",
        "testStrategy": "1. **Unit Tests**: Write unit tests to verify that the streaming response is correctly buffered and processed without repeated text. Use mocking libraries to simulate API responses. \n2. **Integration Tests**: Conduct integration tests with the OpenRouter/DeepSeek API to ensure the fix works in real-world scenarios. Test with different models and edge cases (e.g., network errors). \n3. **Manual Testing**: Perform manual testing to visually inspect the output for any issues. \n4. **Performance Testing**: Run performance tests to ensure that the buffering mechanism does not introduce significant latency or memory usage issues.",
        "status": "done",
        "dependencies": [
          1,
          2,
          6
        ],
        "priority": "medium",
        "subtasks": []
      },
      {
        "id": 8,
        "title": "Fix Token Usage Tracking for OpenRouter/DeepSeek Models",
        "description": "Update the system to accurately track and display token usage for OpenRouter/DeepSeek models, ensuring usage stats reflect actual tokens consumed.",
        "details": "Investigate the current implementation for tracking token usage with OpenRouter/DeepSeek models, focusing on why token counts are reported as zero. Review the API responses from OpenRouter/DeepSeek to determine if token usage information is returned (e.g., in response metadata or headers). If the API does not provide token usage directly, implement logic to estimate tokens based on the prompt and completion using a compatible tokenizer (such as tiktoken or a DeepSeek-specific tokenizer). Integrate this logic into the model usage stats pipeline, ensuring that token counts are updated and displayed correctly in the UI and any relevant logs or analytics. If using a third-party analytics or logging tool (e.g., Langfuse), ensure custom model definitions are set up to enable token cost tracking, as described in community discussions and documentation. Document any changes to the tracking logic and update configuration or environment variables as needed.",
        "testStrategy": "1. Unit test the token counting logic with a variety of prompts and completions to ensure accuracy. 2. Simulate API responses from OpenRouter/DeepSeek and verify that token usage is correctly parsed or estimated. 3. Perform integration tests to confirm that token usage stats are updated in the UI and logs after model invocations. 4. If using analytics tools, verify that token usage and cost are reported as expected for OpenRouter/DeepSeek models. 5. Conduct regression tests to ensure token tracking for other providers remains unaffected.",
        "status": "done",
        "dependencies": [
          1,
          7
        ],
        "priority": "medium",
        "subtasks": []
      },
      {
        "id": 9,
        "title": "Implement Model Discovery Feature",
        "description": "Add --models CLI flag and /models slash command to display available AI models grouped by provider, showing configuration status and model capabilities",
        "details": "",
        "testStrategy": "",
        "status": "pending",
        "dependencies": [
          6
        ],
        "priority": "high",
        "subtasks": [
          {
            "id": 1,
            "title": "Define IProvider Interface with get_available_models()",
            "description": "Create a new IProvider interface that standardizes model discovery by requiring a get_available_models() method for all providers.",
            "dependencies": [],
            "details": "Design the IProvider interface in the shared provider module. The interface should specify a get_available_models() method that returns a list of model metadata objects, including model name, capabilities, and configuration status.",
            "status": "done",
            "testStrategy": "Write unit tests to ensure that any class implementing IProvider must define get_available_models()."
          },
          {
            "id": 2,
            "title": "Implement get_available_models() for Gemini Provider",
            "description": "Add the get_available_models() method to the Gemini provider, returning all supported models with their metadata and configuration status.",
            "dependencies": [
              1
            ],
            "details": "In the Gemini provider implementation, fetch or define the list of available models. For each model, include metadata such as description, context window, pricing, and configuration status (e.g., API key present).",
            "status": "done",
            "testStrategy": "Test that get_available_models() returns accurate model data and correct status based on configuration."
          },
          {
            "id": 3,
            "title": "Implement get_available_models() for OpenRouter Provider",
            "description": "Add the get_available_models() method to the OpenRouter provider, returning all supported models with their metadata and configuration status.",
            "dependencies": [
              1
            ],
            "details": "In the OpenRouter provider, fetch the model list via API or static config. Include metadata (description, context window, pricing) and check for valid API key to set configuration status.",
            "status": "done",
            "testStrategy": "Test with valid and invalid API keys to verify correct model listing and status reporting."
          },
          {
            "id": 4,
            "title": "Implement get_available_models() for Custom API Provider",
            "description": "Add the get_available_models() method to the Custom API provider, returning all supported models with their metadata and configuration status.",
            "dependencies": [
              1
            ],
            "details": "For the Custom API provider, define how models are discovered (e.g., static config or API call). Return model metadata and indicate if required configuration (e.g., endpoint, key) is present.",
            "status": "done",
            "testStrategy": "Test with various custom API configurations to ensure correct model discovery and status."
          },
          {
            "id": 5,
            "title": "Add --models CLI Argument Parsing",
            "description": "Extend the CLI parser to recognize the --models flag and trigger model discovery logic.",
            "dependencies": [
              2,
              3,
              4
            ],
            "details": "Update the CLI argument parser to handle --models. When invoked, aggregate model lists from all providers using their get_available_models() methods.",
            "status": "pending",
            "testStrategy": "Test CLI with --models flag to ensure it triggers model discovery and handles errors gracefully."
          },
          {
            "id": 6,
            "title": "Create Model Listing Output Formatter",
            "description": "Develop a formatter to display models grouped by provider, showing configuration status, capabilities, and metadata.",
            "dependencies": [
              5
            ],
            "details": "Implement a function to format the aggregated model data. Group models by provider, display configuration status (e.g., ✅/❌), and show metadata such as description, context window, and pricing. Ensure output is readable in both CLI and interactive modes.",
            "status": "pending",
            "testStrategy": "Test output formatting with various provider/model combinations and missing configurations."
          },
          {
            "id": 7,
            "title": "Implement /models Slash Command for Interactive Mode",
            "description": "Add a /models command to the interactive CLI that invokes the model discovery logic and displays formatted output.",
            "dependencies": [],
            "details": "Register the /models command in the interactive CLI. When triggered, call the model discovery logic and display the formatted model list to the user.",
            "status": "pending",
            "testStrategy": "Test /models command in interactive mode for correct output and error handling."
          },
          {
            "id": 8,
            "title": "Add Model Aliases Configuration and Comprehensive Tests",
            "description": "Support model aliases in configuration and create tests covering all aspects of the model discovery feature.",
            "dependencies": [],
            "details": "Allow users to define aliases for models in the config. Update model discovery logic to display aliases. Write tests for interface, provider implementations, CLI flag, slash command, output formatting, and alias handling.",
            "status": "pending",
            "testStrategy": "Write unit and integration tests for alias resolution, model listing, and all user-facing features."
          }
        ]
      },
      {
        "id": 10,
        "title": "Enhance Shell Tool Transparency",
        "description": "Improve ShellTool output to show executed command, exit code, full stdout/stderr, and execution duration for better debugging and user confidence",
        "details": "",
        "testStrategy": "",
        "status": "pending",
        "dependencies": [
          9
        ],
        "priority": "medium",
        "subtasks": [
          {
            "id": 1,
            "title": "Modify ShellTool Return Structure",
            "description": "Update the ShellTool's return structure to include the executed command, exit code, full stdout, and stderr outputs.",
            "dependencies": [],
            "details": "Refactor the ShellTool's execution logic so that every run returns a structured object containing the command string, its exit code, and the complete standard output and error streams.",
            "status": "pending",
            "testStrategy": "Write unit tests to verify that the returned object always includes all required fields with accurate values for various command scenarios."
          },
          {
            "id": 2,
            "title": "Implement Execution Duration Tracking",
            "description": "Add logic to measure and record the duration of each command execution within the ShellTool.",
            "dependencies": [
              1
            ],
            "details": "Integrate timing functionality to capture the start and end time of each command, calculating the total execution duration and including it in the return structure.",
            "status": "pending",
            "testStrategy": "Test with commands of varying lengths to ensure the duration is measured accurately and consistently included in the output."
          },
          {
            "id": 3,
            "title": "Update Output Formatting",
            "description": "Revise the ShellTool's output formatting to clearly display the executed command, exit code, stdout, stderr, and execution duration.",
            "dependencies": [
              2
            ],
            "details": "Design and implement a user-friendly output format that presents all transparency details in a readable and organized manner, suitable for both CLI and potential UI consumption.",
            "status": "pending",
            "testStrategy": "Verify output formatting through snapshot and integration tests, ensuring all fields are present and clearly labeled."
          },
          {
            "id": 4,
            "title": "Add Verbosity Settings",
            "description": "Introduce verbosity settings to allow users to control the level of detail shown in ShellTool output.",
            "dependencies": [
              3
            ],
            "details": "Implement configuration options or command-line flags that let users toggle between minimal and detailed output, affecting which fields are displayed.",
            "status": "pending",
            "testStrategy": "Test all verbosity levels to confirm that output detail matches user settings and that toggling works as expected."
          },
          {
            "id": 5,
            "title": "Create Tests for Enhanced Transparency Features",
            "description": "Develop comprehensive tests to validate all new transparency features and ensure robust, reliable behavior.",
            "dependencies": [
              4
            ],
            "details": "Write unit, integration, and regression tests covering all aspects of the enhanced output, including edge cases and error conditions.",
            "status": "pending",
            "testStrategy": "Automate test execution and require all tests to pass before merging changes, ensuring ongoing reliability."
          }
        ]
      },
      {
        "id": 11,
        "title": "Improve Model Listing with Loading Indicators, Intelligent Organization, and Metadata-Based Summaries",
        "description": "Enhance the model listing feature to display loading indicators, organize models intelligently, and provide helpful summaries and recommendations using OpenRouter's model metadata such as pricing, performance, and use cases.",
        "details": "1. Refactor the model listing UI to show a clear loading indicator while fetching models from OpenRouter, ensuring users are informed of data loading states.\n2. Implement intelligent grouping and sorting of models based on key metadata (e.g., provider, pricing tier, performance benchmarks, recommended use cases). Consider using sections or tabs for major providers or categories.\n3. For each model, display a concise summary panel that highlights pricing, performance metrics, and ideal use cases, leveraging metadata from the OpenRouter API. Use badges or icons for quick visual cues (e.g., 'Best for Coding', 'Low Cost', 'High Accuracy').\n4. Add a recommendation engine that suggests models based on user context or common usage patterns (e.g., if the user often selects coding models, highlight those first).\n5. Ensure accessibility and responsiveness in the updated UI, following existing design patterns.\n6. Update data fetching logic to handle errors gracefully and display appropriate messages if metadata is missing or incomplete.\n7. Refactor or extend the existing ModelSelection component (from Task 3) to incorporate these enhancements, ensuring compatibility with the current model selection and state management flows.",
        "testStrategy": "- Verify that the loading indicator appears during model fetch and disappears once data is loaded.\n- Confirm that models are grouped and sorted as intended, with clear visual separation and accurate metadata display.\n- Check that each model's summary includes pricing, performance, and use case information, and that recommendations are contextually relevant.\n- Test error handling by simulating API failures or missing metadata.\n- Ensure the UI remains accessible and responsive across devices.\n- Perform regression testing to confirm that model selection and configuration continue to work as before.",
        "status": "pending",
        "dependencies": [
          3,
          9
        ],
        "priority": "high",
        "subtasks": []
      },
      {
        "id": 12,
        "title": "Update Branding from Google Gemini CLI to Multi-Provider CLI",
        "description": "Rebrand the entire codebase from \"Google Gemini CLI\" to a multi-provider CLI, updating all references in package.json files, README, documentation, CLI output, and hardcoded strings while maintaining backward compatibility where appropriate.",
        "details": "1. **Package.json Updates**: Update all package.json files across the monorepo to reflect the new multi-provider branding. Change package names, descriptions, and metadata. Consider using a neutral name like \"AI CLI\" or \"Multi-AI CLI\" instead of provider-specific branding.\n\n2. **README and Documentation**: \n   - Update the main README.md and all package-specific READMEs to reflect multi-provider support\n   - Change project title, descriptions, and examples to be provider-agnostic\n   - Update installation instructions and usage examples to showcase multiple providers\n   - Ensure OpenRouter, DeepSeek, and other supported providers are prominently mentioned\n\n3. **CLI Output and Messages**:\n   - Search for all hardcoded strings containing \"Gemini\", \"Google Gemini CLI\", or similar references\n   - Update welcome messages, help text, and error messages to reflect multi-provider nature\n   - Update the CLI banner/logo if present to be provider-neutral\n   - Ensure model selection prompts clearly indicate multiple provider support\n\n4. **Code Comments and Internal References**:\n   - Update code comments that reference Gemini-specific functionality\n   - Rename variables, functions, or classes that have \"gemini\" in their names to be provider-agnostic\n   - Update TypeScript interfaces and types to use generic naming conventions\n\n5. **Backward Compatibility Considerations**:\n   - If the CLI was published under a Gemini-specific package name, consider publishing under a new name while deprecating the old one\n   - Maintain any Gemini-specific environment variables (e.g., GEMINI_API_KEY) alongside new generic ones\n   - Add migration guide for users upgrading from the Gemini-specific version\n   - Consider adding aliases for old commands if command names are changing\n\n6. **Configuration Files**:\n   - Update any default configuration files to reflect multi-provider support\n   - Ensure configuration schemas support multiple providers without bias toward Gemini\n\n7. **Build and CI/CD**:\n   - Update build scripts, GitHub Actions workflows, and other CI/CD configurations\n   - Change repository description and topics on GitHub to reflect multi-provider support",
        "testStrategy": "1. **Comprehensive Text Search**: Use grep or similar tools to search for all instances of \"Gemini\", \"Google Gemini\", and related terms across the entire codebase. Verify no references remain after updates.\n\n2. **Package Installation Test**: Test that the package can be installed under its new name and that all functionality works as expected. If maintaining the old package name for compatibility, test that both work correctly.\n\n3. **CLI Output Verification**: Run all CLI commands and verify that output messages, help text, and error messages reflect the multi-provider branding. Take screenshots of key outputs for documentation.\n\n4. **Documentation Review**: Manually review all documentation files to ensure consistent branding and that examples work with multiple providers, not just Gemini.\n\n5. **Backward Compatibility Tests**: \n   - Test that existing Gemini API key environment variables still work\n   - Verify that any deprecated commands show appropriate warnings\n   - Test migration path from old to new configuration format\n\n6. **Integration Tests**: Run the full test suite to ensure no functionality is broken by the rebranding. Pay special attention to provider selection and authentication flows.\n\n7. **Build and Deploy Test**: Verify that the project builds successfully with new branding and that CI/CD pipelines execute without errors. Test package publishing if applicable.",
        "status": "pending",
        "dependencies": [
          6,
          9
        ],
        "priority": "high",
        "subtasks": [
          {
            "id": 1,
            "title": "Update package.json files across monorepo",
            "description": "Update all package.json files to rebrand from 'gemini-cli' to 'sport-cli', including package names, descriptions, bin commands, and metadata",
            "dependencies": [],
            "details": "Search for all package.json files in the monorepo and update: 1) 'name' field from any variation of 'gemini-cli' to 'sport-cli' or '@sport/[package-name]' for scoped packages, 2) 'description' field to reflect multi-provider AI CLI capabilities, 3) 'bin' field to change command from 'gemini' to 'sport', 4) 'keywords' array to include 'multi-provider', 'ai-cli', 'openrouter', 'deepseek' etc., 5) 'homepage', 'repository', and 'bugs' URLs if they contain 'gemini' references",
            "status": "done",
            "testStrategy": "Run 'npm install' and 'npm run build' in each package to ensure package.json changes don't break the build. Verify 'npm link' works correctly with new package names"
          },
          {
            "id": 2,
            "title": "Update CLI command with backward compatibility",
            "description": "Change the main CLI command from 'gemini' to 'sport' while maintaining 'gemini' as a deprecated alias for backward compatibility",
            "dependencies": [
              1
            ],
            "details": "In the main CLI entry point (likely in packages/cli/src/index.ts or similar): 1) Update the primary command registration to use 'sport', 2) Add 'gemini' as an alias that shows a deprecation warning before executing the same functionality, 3) Update commander or yargs configuration to reflect the new command name, 4) Add a deprecation notice that appears when using 'gemini' command suggesting users switch to 'sport'",
            "status": "done",
            "testStrategy": "Test both 'sport' and 'gemini' commands work correctly. Verify deprecation warning appears only for 'gemini' command. Test all subcommands work with both aliases"
          },
          {
            "id": 3,
            "title": "Update all README.md files",
            "description": "Replace all references to 'Gemini CLI' with 'sport-cli' in README files throughout the repository",
            "dependencies": [
              1
            ],
            "details": "Update: 1) Main README.md in repository root - change title, badges, installation instructions, usage examples, 2) Package-specific README files in each package directory, 3) Replace 'Google Gemini CLI' with 'sport-cli - Multi-Provider AI CLI', 4) Update installation commands from 'npm install -g gemini-cli' to 'npm install -g sport-cli', 5) Update all usage examples to use 'sport' command instead of 'gemini', 6) Add section highlighting multi-provider support (OpenRouter, DeepSeek, etc.)",
            "status": "pending",
            "testStrategy": "Use grep to ensure no 'Gemini CLI' or 'gemini-cli' references remain in README files. Manually review formatting and ensure all examples are updated"
          },
          {
            "id": 4,
            "title": "Update documentation in docs/ folder",
            "description": "Update all documentation files to reflect the rebranding from Gemini CLI to sport-cli",
            "dependencies": [
              3
            ],
            "details": "Search docs/ folder for all .md, .mdx, or other documentation files and update: 1) Replace 'Gemini CLI' with 'sport-cli' in all text content, 2) Update code examples to use 'sport' command, 3) Update configuration examples to use new paths and environment variables, 4) Update any diagrams or images that contain 'Gemini' branding, 5) Ensure API documentation reflects multi-provider support, 6) Update any provider-specific guides to be provider-agnostic",
            "status": "pending",
            "testStrategy": "Build documentation site (if applicable) and verify all links work. Search for remaining 'gemini' references. Review generated documentation for consistency"
          },
          {
            "id": 5,
            "title": "Update hardcoded strings in source code",
            "description": "Find and replace all hardcoded 'Gemini CLI' references in TypeScript/JavaScript source files",
            "dependencies": [
              2
            ],
            "details": "Search all .ts, .tsx, .js, .jsx files for: 1) String literals containing 'Gemini CLI', 'Google Gemini CLI', or similar, 2) Update CLI welcome messages, help text, error messages to use 'sport-cli', 3) Update any console.log statements, 4) Update error messages to be provider-agnostic, 5) Search for 'gemini' in variable names, function names, and class names - rename to generic terms like 'provider', 'ai', or 'sport', 6) Update any hardcoded URLs or API endpoints that reference Gemini",
            "status": "pending",
            "testStrategy": "Run the CLI and verify all user-facing text is updated. Use automated tests to check error messages. Grep for remaining 'gemini' references in source code"
          },
          {
            "id": 6,
            "title": "Migrate configuration directory with backward compatibility",
            "description": "Change configuration directory from .gemini to .sport while providing automatic migration for existing users",
            "dependencies": [
              5
            ],
            "details": "Implement configuration migration: 1) Update default config directory path from '~/.gemini' to '~/.sport', 2) On startup, check if .gemini exists but .sport doesn't - if so, copy/migrate configuration, 3) Update all file I/O operations to use new path, 4) Create migration function that preserves user settings, API keys, and preferences, 5) Log migration status to inform users, 6) Update config file names if they contain 'gemini' (e.g., gemini-config.json to sport-config.json)",
            "status": "pending",
            "testStrategy": "Test fresh installation creates .sport directory. Test existing .gemini directory is migrated correctly. Verify all settings are preserved after migration"
          },
          {
            "id": 7,
            "title": "Update environment variables with backward compatibility",
            "description": "Change environment variable prefixes from GEMINI_ to SPORT_ while maintaining support for old variables",
            "dependencies": [
              6
            ],
            "details": "Update environment variable handling: 1) Change all GEMINI_ prefixed variables to SPORT_ (e.g., GEMINI_API_KEY to SPORT_API_KEY), 2) Implement fallback logic that checks for old variables if new ones aren't set, 3) Add deprecation warnings when old variables are used, 4) Update .env.example files with new variable names, 5) Document both old and new variables in configuration guide, 6) Consider generic names like SPORT_OPENROUTER_API_KEY for provider-specific keys",
            "status": "pending",
            "testStrategy": "Test both old and new environment variables work. Verify deprecation warnings appear for old variables. Test precedence when both are set"
          },
          {
            "id": 8,
            "title": "Update CLI help text and command descriptions",
            "description": "Update all help text, command descriptions, and usage information to reflect sport-cli branding",
            "dependencies": [
              5,
              7
            ],
            "details": "Update command help text: 1) Main help banner should show 'sport-cli' and describe multi-provider capabilities, 2) Update all command descriptions to be provider-agnostic, 3) Update examples in help text to use 'sport' command, 4) Ensure --help output for all subcommands is updated, 5) Update version command output to show 'sport-cli', 6) Add provider list to help text showing supported providers (OpenRouter, DeepSeek, etc.)",
            "status": "pending",
            "testStrategy": "Run 'sport --help' and all subcommand help options. Verify all text is updated and no 'gemini' references remain. Check help text formatting is correct"
          },
          {
            "id": 9,
            "title": "Update license headers and copyright notices",
            "description": "Update any license headers, copyright notices, and legal text to reflect the new sport-cli branding if needed",
            "dependencies": [
              8
            ],
            "details": "Review and update: 1) File headers in source code files that mention 'Gemini CLI', 2) LICENSE file if it contains specific product references, 3) Copyright notices in documentation, 4) Any third-party attribution files, 5) Update author/maintainer information if needed, 6) Ensure open source compliance for multi-provider support",
            "status": "pending",
            "testStrategy": "Grep for copyright notices and license headers. Verify LICENSE file is appropriate for multi-provider CLI. Check all legal requirements are met"
          },
          {
            "id": 10,
            "title": "Create comprehensive migration guide",
            "description": "Create a migration guide for users upgrading from the Gemini-specific version to sport-cli",
            "dependencies": [
              1,
              2,
              3,
              4,
              5,
              6,
              7,
              8,
              9
            ],
            "details": "Create MIGRATION.md file containing: 1) Overview of changes from gemini-cli to sport-cli, 2) Step-by-step upgrade instructions, 3) Command mapping table (gemini -> sport), 4) Environment variable mapping (GEMINI_* -> SPORT_*), 5) Configuration directory migration (.gemini -> .sport), 6) Breaking changes and how to address them, 7) Benefits of multi-provider support, 8) FAQ section for common migration issues, 9) Rollback instructions if needed, 10) Timeline for deprecation of old commands/variables",
            "status": "pending",
            "testStrategy": "Follow the migration guide on a system with old gemini-cli installed. Verify all steps work correctly. Have team members review for clarity and completeness"
          }
        ]
      },
      {
        "id": 13,
        "title": "Fix CLI Test Failures for Slash Commands and Compression",
        "description": "Resolve test failures in the CLI package specifically related to slash command processing and compression functionality, excluding any failures caused by branding changes.",
        "details": "1. **Identify Failing Tests**: Run the test suite for the CLI package and identify all failing tests related to slash command processing and compression. Filter out any failures that are due to branding changes (Task #12).\n\n2. **Slash Command Processing Fixes**:\n   - Review test files in `/packages/cli/src/commands/` and `/packages/cli/src/ui/hooks/` directories\n   - Common issues to check:\n     - Command parsing logic expecting specific formats or delimiters\n     - Command handler registration and routing\n     - Async command execution and promise handling\n     - Input validation and error handling for malformed commands\n   - Update command processors to handle edge cases like empty commands, special characters, or concurrent command execution\n\n3. **Compression-Related Fixes**:\n   - Locate compression-related tests (likely in utilities or middleware)\n   - Common compression issues:\n     - Incorrect compression/decompression of command outputs\n     - Buffer handling for large outputs\n     - Encoding issues with compressed data\n     - Stream processing for real-time compression\n   - Ensure compression works correctly with various data types (text, JSON, binary)\n\n4. **Test Environment Setup**:\n   - Verify test environment configuration matches production settings\n   - Check for missing mock implementations or stubs\n   - Ensure test fixtures are up-to-date with current data structures\n   - Review test timeouts for async operations\n\n5. **Code Updates**:\n   - Fix implementation code based on test expectations\n   - Update deprecated API calls or library methods\n   - Ensure proper error handling and edge case coverage\n   - Add missing type definitions if using TypeScript\n\n6. **Test Refactoring** (if needed):\n   - Update outdated test assertions\n   - Fix flaky tests by adding proper wait conditions\n   - Improve test isolation to prevent cross-test contamination\n   - Add missing test cases discovered during debugging",
        "testStrategy": "1. **Baseline Test Run**: Execute `npm test` or `yarn test` in the CLI package directory and document all failing tests related to slash commands and compression. Create a checklist of failures to track progress.\n\n2. **Isolated Test Execution**: Run failing tests individually using test runners' focused mode (e.g., `test.only()` or `--testNamePattern`) to isolate issues and reduce noise from other tests.\n\n3. **Debug Mode Testing**: Run tests with verbose logging or debug flags to capture detailed error messages and stack traces. Use debugger breakpoints in both test and implementation code.\n\n4. **Regression Testing**: After fixing each test, run the entire test suite to ensure no new failures are introduced. Pay special attention to related functionality that might be affected.\n\n5. **Edge Case Verification**:\n   - Test slash commands with special characters: `/test!@#$`, `/cmd with spaces`\n   - Test compression with various data sizes: empty, small (< 1KB), medium (1MB), large (> 10MB)\n   - Test error scenarios: invalid commands, compression failures, timeout conditions\n\n6. **Integration Testing**: Verify that fixed functionality works correctly in the actual CLI environment, not just in unit tests. Manually test key slash commands and compression features.\n\n7. **Performance Testing**: Ensure that fixes don't introduce performance regressions, especially for compression operations on large datasets.\n\n8. **CI/CD Verification**: Confirm all tests pass in the continuous integration environment, which may have different configurations than local development.",
        "status": "pending",
        "dependencies": [
          9
        ],
        "priority": "medium",
        "subtasks": []
      },
      {
        "id": 14,
        "title": "Fix OpenRouter API Error for Invalid Model ID",
        "description": "Resolve the error where 'gemini-2.5-pro' is not recognized as a valid model ID by OpenRouter, and update the default model configuration to use a valid OpenRouter-supported model.",
        "details": "1. **Identify the Issue Location**: Search the codebase for hardcoded references to 'gemini-2.5-pro' model ID, particularly in:\n   - Default configuration files (config.json, .env.example, etc.)\n   - Model selection logic in provider-specific implementations\n   - CLI initialization code where default models are set\n   - Any fallback model configurations\n\n2. **Verify Valid OpenRouter Models**: \n   - Check OpenRouter's API documentation or use their models endpoint to get a list of valid model IDs\n   - Common valid OpenRouter models include: 'openai/gpt-4', 'anthropic/claude-2', 'meta-llama/llama-2-70b-chat', 'google/palm-2-chat-bison', etc.\n   - Choose an appropriate default model that balances capability and cost\n\n3. **Update Default Configuration**:\n   - Replace 'gemini-2.5-pro' with a valid OpenRouter model ID (e.g., 'openai/gpt-3.5-turbo' for a cost-effective default)\n   - Ensure the replacement model is consistently available and well-supported\n   - Update any configuration schemas or TypeScript interfaces that define model IDs\n\n4. **Implement Model Validation**:\n   - Add a validation function that checks if a model ID is valid for the selected provider\n   - Consider caching the list of valid models from OpenRouter to avoid repeated API calls\n   - Provide helpful error messages when an invalid model is specified\n\n5. **Update Documentation**:\n   - Update any documentation that references the old default model\n   - Add notes about OpenRouter's model naming convention (provider/model-name format)\n   - Include examples of valid OpenRouter model IDs in configuration documentation\n\n6. **Consider Provider-Specific Defaults**:\n   - Implement a mapping of provider-specific default models\n   - For OpenRouter, use a widely available model like 'openai/gpt-3.5-turbo'\n   - For direct provider connections, use their native model IDs",
        "testStrategy": "1. **Unit Tests for Model Validation**:\n   - Create tests that verify the model validation function correctly identifies valid and invalid OpenRouter model IDs\n   - Test that appropriate error messages are returned for invalid models\n   - Mock the OpenRouter API response for model listing to ensure consistent testing\n\n2. **Integration Tests with OpenRouter**:\n   - Test making actual API calls with the new default model to ensure it works correctly\n   - Verify that the model switch from 'gemini-2.5-pro' to the new default doesn't break existing functionality\n   - Test error handling when an invalid model is specified in configuration\n\n3. **Configuration Tests**:\n   - Verify that the default configuration loads the correct OpenRouter model\n   - Test that environment variable overrides work correctly with the new model ID\n   - Ensure backward compatibility for users who might have custom configurations\n\n4. **CLI Flow Testing**:\n   - Manually test the CLI with OpenRouter provider to ensure no 'invalid model' errors occur\n   - Test model selection flow to verify users can see and select valid OpenRouter models\n   - Verify that helpful error messages appear if users try to use incompatible model IDs\n\n5. **Regression Testing**:\n   - Run the full test suite to ensure the model change doesn't break other functionality\n   - Pay special attention to any tests that might have hardcoded the old model ID\n   - Test with multiple providers to ensure the fix doesn't affect non-OpenRouter providers",
        "status": "pending",
        "dependencies": [
          1,
          6
        ],
        "priority": "high",
        "subtasks": []
      },
      {
        "id": 15,
        "title": "Fix Terminal Bold Formatting for Model Names",
        "description": "Implement proper ANSI escape code handling to render bold text correctly in terminal output instead of showing raw markdown asterisks",
        "details": "Research indicates the issue stems from MessageType.GEMINI not properly converting markdown bold syntax to ANSI codes. Implement a markdown-to-ANSI converter using chalk@5.3.0 or ansi-styles@6.2.1. Create a utility function that detects **text** patterns and replaces with \\x1b[1mtext\\x1b[0m sequences. For cross-platform compatibility, use supports-color@9.4.0 to detect terminal capabilities. Implementation: \n\n```typescript\nimport chalk from 'chalk';\nimport supportsColor from 'supports-color';\n\nfunction formatBoldText(text: string): string {\n  if (!supportsColor.stdout) return text.replace(/\\*\\*(.*?)\\*\\*/g, '$1');\n  return text.replace(/\\*\\*(.*?)\\*\\*/g, (_, p1) => chalk.bold(p1));\n}\n```\n\nIntegrate this into the MessageType.GEMINI rendering pipeline to ensure all table content processes through this formatter.",
        "testStrategy": "Create unit tests with various markdown bold patterns including edge cases like nested asterisks, incomplete patterns, and mixed content. Test on different terminal emulators (Windows Terminal, iTerm2, standard Linux terminals) to ensure cross-platform compatibility. Verify that when terminal doesn't support colors, bold markers are cleanly removed.",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 16,
        "title": "Implement Dynamic Column Width Calculation for Table Display",
        "description": "Create an intelligent column width system that prevents truncation of headers like 'Context' and adjusts to terminal width constraints",
        "details": "Implement a dynamic column width calculator using terminal-size@3.0.0 to detect available width. Research shows cli-table3@0.6.3 provides excellent table rendering with automatic width calculation. Implementation approach:\n\n```typescript\nimport Table from 'cli-table3';\nimport terminalSize from 'terminal-size';\n\nfunction createModelTable(models: Model[]): string {\n  const { columns } = terminalSize();\n  const table = new Table({\n    head: ['Model', 'Context', 'Cost', 'Best For'],\n    colWidths: [\n      Math.floor(columns * 0.35), // 35% for model name\n      Math.floor(columns * 0.15), // 15% for context\n      Math.floor(columns * 0.20), // 20% for cost\n      Math.floor(columns * 0.30)  // 30% for best for\n    ],\n    wordWrap: true,\n    wrapOnWordBoundary: true\n  });\n  \n  // Add responsive breakpoints\n  if (columns < 80) {\n    // Compact mode for narrow terminals\n    table.options.colWidths = [25, 10, 15, 30];\n  }\n  \n  return table.toString();\n}\n```\n\nEnsure minimum column widths to prevent 'Con...' truncation. Add ellipsis handling for extremely long content while preserving critical information.",
        "testStrategy": "Test with various terminal widths from 60 to 200 columns. Verify that 'Context' header never truncates to 'Con...'. Create test cases with extremely long model names and descriptions to ensure graceful handling. Mock terminal-size to test different viewport scenarios. Validate table alignment remains consistent across all widths.",
        "priority": "high",
        "dependencies": [
          15
        ],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 17,
        "title": "Build Intelligent Model Deduplication and Curation System",
        "description": "Implement logic to select exactly one best Dolphin model plus one flagship model from each major provider, eliminating duplicate recommendations",
        "details": "Create a sophisticated model curation system that groups models by provider and selects the best from each category. Use a scoring algorithm based on context length, cost efficiency, and capability markers. Implementation:\n\n```typescript\ninterface ModelScore {\n  model: Model;\n  score: number;\n  category: 'dolphin' | 'openai' | 'anthropic' | 'google' | 'xai' | 'other';\n}\n\nfunction curateModels(allModels: Model[]): Model[] {\n  const categorized = new Map<string, ModelScore[]>();\n  \n  allModels.forEach(model => {\n    const category = detectCategory(model.id);\n    const score = calculateScore(model);\n    \n    if (!categorized.has(category)) {\n      categorized.set(category, []);\n    }\n    categorized.get(category)!.push({ model, score, category });\n  });\n  \n  // Select best from each category\n  const selected: Model[] = [];\n  \n  // Prioritize Dolphin\n  const dolphinModels = categorized.get('dolphin') || [];\n  if (dolphinModels.length > 0) {\n    const best = dolphinModels.sort((a, b) => b.score - a.score)[0];\n    selected.push(best.model);\n  }\n  \n  // Add one from each major provider\n  ['openai', 'anthropic', 'google', 'xai'].forEach(provider => {\n    const models = categorized.get(provider) || [];\n    if (models.length > 0) {\n      const best = models.sort((a, b) => b.score - a.score)[0];\n      selected.push(best.model);\n    }\n  });\n  \n  return selected;\n}\n\nfunction calculateScore(model: Model): number {\n  let score = 0;\n  \n  // Context length scoring (logarithmic scale)\n  score += Math.log10(model.contextLength) * 10;\n  \n  // Cost efficiency (inverse relationship)\n  const avgCost = (model.inputCost + model.outputCost) / 2;\n  score += 100 / (avgCost + 1);\n  \n  // Boost for newer models\n  if (model.id.includes('4') || model.id.includes('3.5')) score += 20;\n  if (model.id.includes('turbo')) score += 10;\n  \n  return score;\n}\n```",
        "testStrategy": "Create comprehensive test suite with mock model data including multiple Dolphin variants, various GPT models, Claude versions, and Gemini models. Verify exactly one model per provider is selected. Test edge cases like missing providers, single model scenarios, and identical scoring situations. Validate that the highest-scoring Dolphin model is always selected when available.",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 18,
        "title": "Implement Cost Display Formatting and Explanation System",
        "description": "Create a clear, consistent cost display format with complete explanation text that doesn't get cut off",
        "details": "Design a cost formatting system that displays prices clearly and includes a comprehensive explanation. Use Intl.NumberFormat for locale-aware formatting and implement smart unit selection (per 1K or 1M tokens based on magnitude). Implementation:\n\n```typescript\nfunction formatCost(inputCost: number, outputCost: number): string {\n  const formatter = new Intl.NumberFormat('en-US', {\n    style: 'currency',\n    currency: 'USD',\n    minimumFractionDigits: 2,\n    maximumFractionDigits: 4\n  });\n  \n  // Determine optimal unit (1K vs 1M tokens)\n  const avgCost = (inputCost + outputCost) / 2;\n  const useMillions = avgCost < 0.01;\n  \n  if (useMillions) {\n    const inCost = formatter.format(inputCost * 1000);\n    const outCost = formatter.format(outputCost * 1000);\n    return `${inCost}/${outCost} per 1M`;\n  } else {\n    const inCost = formatter.format(inputCost);\n    const outCost = formatter.format(outputCost);\n    return `${inCost}/${outCost} per 1K`;\n  }\n}\n\nfunction generateCostExplanation(): string {\n  return `\n💰 Cost Format: $input/$output per 1K tokens\n   • Input cost: Price for text you send to the model\n   • Output cost: Price for text generated by the model\n   • 1K tokens ≈ 750 words\n   • Prices from OpenRouter as of ${new Date().toLocaleDateString()}\n`.trim();\n}\n```\n\nEnsure explanation text appears below the table with proper spacing and doesn't get truncated by terminal width constraints.",
        "testStrategy": "Test cost formatting with various price ranges from $0.0001 to $10.00. Verify automatic unit switching between 1K and 1M tokens. Test explanation text rendering at different terminal widths to ensure no truncation. Validate currency formatting works correctly with different locales. Create snapshot tests for consistent output format.",
        "priority": "medium",
        "dependencies": [
          16
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 19,
        "title": "Create Enhanced Model Metadata and 'Best For' Descriptions",
        "description": "Implement a system to generate meaningful, specific 'Best For' descriptions for each model based on their characteristics and capabilities",
        "details": "Build a metadata enrichment system that analyzes model capabilities and generates targeted use-case descriptions. Use a combination of model ID parsing, capability detection, and predefined templates. Implementation:\n\n```typescript\ninterface ModelMetadata {\n  strengths: string[];\n  bestFor: string;\n  category: string;\n}\n\nfunction generateModelMetadata(model: Model): ModelMetadata {\n  const metadata: ModelMetadata = {\n    strengths: [],\n    bestFor: '',\n    category: detectModelCategory(model.id)\n  };\n  \n  // Analyze context length\n  if (model.contextLength >= 128000) {\n    metadata.strengths.push('long documents');\n  } else if (model.contextLength >= 32000) {\n    metadata.strengths.push('extended conversations');\n  }\n  \n  // Analyze cost efficiency\n  const avgCost = (model.inputCost + model.outputCost) / 2;\n  if (avgCost < 0.001) {\n    metadata.strengths.push('high-volume tasks');\n  }\n  \n  // Model-specific capabilities\n  const templates: Record<string, string> = {\n    'dolphin': 'Uncensored coding & technical tasks',\n    'gpt-4': 'Complex reasoning & analysis',\n    'claude-3': 'Long-form content & code review',\n    'gemini': 'Multimodal tasks & large contexts',\n    'grok': 'Real-time info & casual coding'\n  };\n  \n  // Generate 'Best For' description\n  const baseTemplate = templates[metadata.category] || 'General purpose';\n  const strengths = metadata.strengths.slice(0, 2).join(', ');\n  \n  metadata.bestFor = strengths \n    ? `${baseTemplate}, ${strengths}`\n    : baseTemplate;\n    \n  // Ensure description fits in column width\n  if (metadata.bestFor.length > 40) {\n    metadata.bestFor = metadata.bestFor.substring(0, 37) + '...';\n  }\n  \n  return metadata;\n}\n```\n\nIntegrate with the model display system to show contextual, valuable information for each model.",
        "testStrategy": "Create test cases for each model category with various context lengths and cost profiles. Verify descriptions are meaningful and differentiated. Test truncation logic for long descriptions. Validate that each major model gets an appropriate, specific description. Use snapshot testing to ensure consistency in description generation.",
        "priority": "medium",
        "dependencies": [
          17
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 20,
        "title": "Add Usage Examples and Model List Navigation",
        "description": "Implement helpful usage examples and links to comprehensive model lists, enhancing the command's utility and discoverability",
        "details": "Create a comprehensive help section that appears after the model table, providing usage examples and navigation options. Use terminal-link@3.0.0 for clickable URLs in supported terminals. Implementation:\n\n```typescript\nimport terminalLink from 'terminal-link';\n\nfunction generateUsageSection(): string {\n  const sections = [];\n  \n  // Usage examples\n  sections.push('📝 Usage Examples:');\n  sections.push('  sport-cli chat --model dolphin-mixtral     # Start chat with Dolphin');\n  sections.push('  sport-cli ask \"Fix this bug\" --model gpt-4  # One-shot query');\n  sections.push('  sport-cli config set default-model claude-3  # Set default');\n  sections.push('');\n  \n  // Links section\n  sections.push('🔗 More Options:');\n  \n  const fullListUrl = 'https://openrouter.ai/models';\n  const docsUrl = 'https://sport-cli.dev/models';\n  \n  if (terminalLink.isSupported) {\n    sections.push(`  • ${terminalLink('View all 200+ models', fullListUrl)}`);\n    sections.push(`  • ${terminalLink('Model comparison guide', docsUrl)}`);\n  } else {\n    sections.push(`  • View all 200+ models: ${fullListUrl}`);\n    sections.push(`  • Model comparison guide: ${docsUrl}`);\n  }\n  \n  sections.push('');\n  sections.push('💡 Tip: Use \"sport-cli models --all\" to see extended list');\n  \n  return sections.join('\\n');\n}\n\n// Add command flag handling\ninterface ModelCommandOptions {\n  all?: boolean;\n  category?: string;\n  maxCost?: number;\n  minContext?: number;\n}\n\nfunction handleModelCommand(options: ModelCommandOptions): void {\n  if (options.all) {\n    displayAllModels();\n  } else {\n    displayCuratedModels();\n    console.log(generateUsageSection());\n  }\n}\n```\n\nEnsure the usage section is visually distinct from the table and provides immediate value to users.",
        "testStrategy": "Test terminal link support detection and fallback behavior. Verify usage examples are accurate and executable. Test with various terminal emulators to ensure links work where supported. Validate that the help section doesn't interfere with table display. Create integration tests that verify the complete command output including table, explanation, and usage sections.",
        "priority": "low",
        "dependencies": [
          15,
          16,
          17,
          18,
          19
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 21,
        "title": "Create Task Analysis System for AI Model Selection",
        "description": "Implement a system that evaluates each task's complexity, requirements, and recommends the optimal AI model for implementation based on factors like code complexity, reasoning requirements, context needs, and cost-effectiveness",
        "details": "Build a comprehensive task analysis system that intelligently recommends AI models for specific tasks. The system should analyze multiple dimensions of task complexity and match them to model capabilities. Implementation approach:\n\n```typescript\ninterface TaskAnalysis {\n  taskId: number;\n  complexity: {\n    codeComplexity: 'low' | 'medium' | 'high';\n    reasoningDepth: 'shallow' | 'moderate' | 'deep';\n    contextRequirement: number; // estimated tokens needed\n    creativityNeeded: boolean;\n    precisionRequired: boolean;\n  };\n  recommendedModel: {\n    primary: string;\n    alternative: string;\n    rationale: string;\n  };\n  costEstimate: {\n    estimatedTokens: number;\n    costWithPrimary: number;\n    costWithAlternative: number;\n  };\n}\n\nclass TaskAnalyzer {\n  private modelCapabilities: Map<string, ModelProfile>;\n  \n  analyzeTask(task: Task): TaskAnalysis {\n    const complexity = this.assessComplexity(task);\n    const contextNeeds = this.estimateContextRequirement(task);\n    const modelRecommendation = this.recommendModel(complexity, contextNeeds);\n    \n    return {\n      taskId: task.id,\n      complexity,\n      recommendedModel: modelRecommendation,\n      costEstimate: this.calculateCosts(task, modelRecommendation)\n    };\n  }\n  \n  private assessComplexity(task: Task): TaskComplexity {\n    // Analyze task description for keywords indicating complexity\n    const complexityIndicators = {\n      high: ['algorithm', 'optimization', 'architecture', 'system design', 'complex'],\n      medium: ['implement', 'integrate', 'refactor', 'enhance'],\n      low: ['update', 'fix', 'add', 'simple', 'basic']\n    };\n    \n    // Check for reasoning depth requirements\n    const reasoningIndicators = {\n      deep: ['analyze', 'evaluate', 'design', 'architect', 'strategy'],\n      moderate: ['implement', 'build', 'create', 'develop'],\n      shallow: ['update', 'modify', 'fix', 'adjust']\n    };\n    \n    // Scan task details for code blocks and technical depth\n    const codeBlockCount = (task.details.match(/```/g) || []).length / 2;\n    const hasArchitecturalDecisions = /interface|class|architecture|design pattern/i.test(task.details);\n    \n    return {\n      codeComplexity: this.categorizeCodeComplexity(codeBlockCount, hasArchitecturalDecisions),\n      reasoningDepth: this.categorizeReasoningDepth(task.description + task.details),\n      contextRequirement: this.estimateTokens(task),\n      creativityNeeded: /design|create|innovative|novel/i.test(task.description),\n      precisionRequired: /critical|exact|precise|accurate/i.test(task.description)\n    };\n  }\n  \n  private recommendModel(complexity: TaskComplexity, contextNeeds: number): ModelRecommendation {\n    // Model selection logic based on task requirements\n    if (complexity.reasoningDepth === 'deep' || complexity.codeComplexity === 'high') {\n      return {\n        primary: 'o3-mini', // or 'claude-3-opus' for deep reasoning\n        alternative: 'claude-3-sonnet',\n        rationale: 'Complex task requiring deep reasoning and sophisticated code generation'\n      };\n    }\n    \n    if (complexity.reasoningDepth === 'moderate' && contextNeeds < 50000) {\n      return {\n        primary: 'claude-3-sonnet',\n        alternative: 'gpt-4-turbo',\n        rationale: 'Balanced task requiring good reasoning with moderate context'\n      };\n    }\n    \n    if (complexity.codeComplexity === 'low' && !complexity.creativityNeeded) {\n      return {\n        primary: 'gemini-1.5-flash',\n        alternative: 'claude-3-haiku',\n        rationale: 'Simple task suitable for fast, efficient models'\n      };\n    }\n    \n    // Context-heavy tasks\n    if (contextNeeds > 100000) {\n      return {\n        primary: 'gemini-1.5-pro', // 2M context\n        alternative: 'claude-3-sonnet',\n        rationale: 'Task requires extensive context window'\n      };\n    }\n    \n    // Default balanced recommendation\n    return {\n      primary: 'claude-3-sonnet',\n      alternative: 'gpt-4-turbo',\n      rationale: 'Standard development task with balanced requirements'\n    };\n  }\n}\n\n// Integration with existing model system\nfunction integrateWithModelCommand(analyzer: TaskAnalyzer): void {\n  // Add task analysis results to model selection UI\n  const analysisResults = analyzer.analyzeAllTasks();\n  \n  // Display recommendations in model list\n  console.log('📊 Task-Optimized Model Recommendations:');\n  analysisResults.forEach(analysis => {\n    console.log(`Task #${analysis.taskId}: ${analysis.recommendedModel.primary}`);\n    console.log(`  Rationale: ${analysis.recommendedModel.rationale}`);\n    console.log(`  Est. Cost: ${formatCurrency(analysis.costEstimate.costWithPrimary)}`);\n  });\n}\n```\n\nThe system should also include a CLI interface for analyzing specific tasks:\n\n```bash\nsport-cli analyze-task 21  # Analyze specific task\nsport-cli analyze-all      # Analyze all tasks and generate report\nsport-cli recommend --task \"implement caching system\"  # Get model recommendation for description\n```",
        "testStrategy": "Create comprehensive test suite covering all aspects of the task analysis system:\n\n1. **Complexity Assessment Tests**: Create test cases with various task descriptions representing different complexity levels. Verify that code complexity detection correctly identifies simple updates vs. architectural changes. Test reasoning depth categorization with tasks ranging from simple fixes to system design.\n\n2. **Context Estimation Tests**: Test token estimation accuracy by comparing with actual tokenizer counts for various task sizes. Create edge cases with very large tasks (>100k tokens) and minimal tasks (<1k tokens). Verify context requirement calculations include dependencies and related code.\n\n3. **Model Recommendation Tests**: Test recommendation logic with tasks of varying complexity profiles. Verify that high-complexity tasks recommend powerful models (O3/Opus), medium tasks recommend balanced models (Sonnet), and simple tasks recommend fast models (Flash/Haiku). Test edge cases where multiple factors conflict (e.g., simple logic but huge context).\n\n4. **Cost Calculation Tests**: Verify cost estimates are accurate based on model pricing and estimated token usage. Test that alternative model costs are calculated correctly. Create scenarios with different token volumes to ensure pricing tiers are handled properly.\n\n5. **Integration Tests**: Test CLI commands with mock task data. Verify analyze-task command produces correct output format. Test analyze-all generates comprehensive reports. Validate that recommendations integrate properly with existing model selection system.\n\n6. **Performance Tests**: Ensure analysis of large task sets (100+ tasks) completes in reasonable time. Test memory usage with extensive task histories. Verify caching mechanisms work for repeated analyses.",
        "status": "pending",
        "dependencies": [
          17,
          19
        ],
        "priority": "high",
        "subtasks": []
      },
      {
        "id": 22,
        "title": "Create Model Capability Registry System",
        "description": "Design and implement a centralized registry to track which AI models support tool/function calling capabilities",
        "details": "Create a new file `/packages/core/src/providers/modelCapabilities.ts` that exports:\n\n```typescript\ninterface ModelCapability {\n  modelId: string;\n  provider: string;\n  supportsTools: boolean;\n  maxTokens?: number;\n  lastUpdated: Date;\n}\n\nclass ModelCapabilityRegistry {\n  private capabilities: Map<string, ModelCapability>;\n  private cache: Map<string, boolean>;\n  \n  constructor() {\n    this.capabilities = new Map();\n    this.cache = new Map();\n    this.initializeKnownModels();\n  }\n  \n  private initializeKnownModels() {\n    // Known models with tool support\n    const toolSupportModels = [\n      'gpt-4', 'gpt-4-turbo', 'gpt-3.5-turbo',\n      'claude-3-opus', 'claude-3-sonnet', 'claude-3-haiku',\n      'gemini-pro', 'gemini-1.5-pro'\n    ];\n    \n    // Known models without tool support\n    const noToolSupportModels = [\n      'grok-beta', 'dolphin-mistral', 'llama-2-70b',\n      'mixtral-8x7b', 'yi-34b'\n    ];\n  }\n  \n  supportsTools(modelId: string): boolean {\n    if (this.cache.has(modelId)) {\n      return this.cache.get(modelId)!;\n    }\n    // Check registry and cache result\n  }\n}\n\nexport const modelCapabilityRegistry = new ModelCapabilityRegistry();\n```",
        "testStrategy": "Unit tests should verify:\n1. Registry correctly identifies known models with/without tool support\n2. Cache functionality works and improves performance\n3. Unknown models default to false (no tool support)\n4. Registry can be updated dynamically\n\nCreate test file: `/packages/core/src/providers/__tests__/modelCapabilities.test.ts`",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 23,
        "title": "Add Capability Checking to OpenRouterContentGenerator",
        "description": "Modify OpenRouterContentGenerator to check model capabilities before including tools in API requests",
        "details": "Update `/packages/core/src/providers/openRouterContentGenerator.ts`:\n\n```typescript\nimport { modelCapabilityRegistry } from './modelCapabilities';\n\nclass OpenRouterContentGenerator {\n  async generateContent(messages: Message[], options: GenerateOptions) {\n    const modelId = options.model || this.defaultModel;\n    const supportsTools = modelCapabilityRegistry.supportsTools(modelId);\n    \n    const requestBody: any = {\n      model: modelId,\n      messages: this.formatMessages(messages),\n      stream: options.stream || false,\n      temperature: options.temperature || 0.7,\n      max_tokens: options.maxTokens || 4096\n    };\n    \n    // Only include tools if model supports them\n    if (supportsTools && options.tools && options.tools.length > 0) {\n      requestBody.tools = this.formatTools(options.tools);\n      requestBody.tool_choice = options.toolChoice || 'auto';\n    }\n    \n    try {\n      const response = await this.makeRequest(requestBody);\n      return response;\n    } catch (error) {\n      // Handle 404 \"No endpoints found\" error\n      if (error.status === 404 && error.message?.includes('No endpoints found')) {\n        console.warn(`Model ${modelId} doesn't support tools, retrying without...`);\n        delete requestBody.tools;\n        delete requestBody.tool_choice;\n        return await this.makeRequest(requestBody);\n      }\n      throw error;\n    }\n  }\n}\n```",
        "testStrategy": "Integration tests should verify:\n1. Tools are included for GPT-4, Claude models\n2. Tools are excluded for Grok, Dolphin-Mistral\n3. 404 errors trigger automatic retry without tools\n4. Successful fallback behavior when tools are removed\n5. Performance impact is minimal with caching",
        "priority": "high",
        "dependencies": [
          22
        ],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 24,
        "title": "Update CustomApiContentGenerator with Capability Detection",
        "description": "Add similar capability checking logic to CustomApiContentGenerator for consistency across providers",
        "details": "Modify `/packages/core/src/providers/customApiContentGenerator.ts`:\n\n```typescript\nimport { modelCapabilityRegistry } from './modelCapabilities';\n\nclass CustomApiContentGenerator {\n  async generateContent(messages: Message[], options: GenerateOptions) {\n    const modelId = this.config.model || options.model;\n    const supportsTools = modelCapabilityRegistry.supportsTools(modelId);\n    \n    // Build request based on API format (OpenAI, Anthropic, etc.)\n    const request = this.buildRequest(messages, options);\n    \n    // Conditionally add tools\n    if (supportsTools && options.tools?.length > 0) {\n      if (this.config.apiFormat === 'openai') {\n        request.tools = options.tools;\n        request.tool_choice = options.toolChoice || 'auto';\n      } else if (this.config.apiFormat === 'anthropic') {\n        request.tools = this.formatAnthropicTools(options.tools);\n      }\n    }\n    \n    try {\n      return await this.sendRequest(request);\n    } catch (error) {\n      if (this.isToolNotSupportedError(error)) {\n        console.warn('Tools not supported, retrying without...');\n        delete request.tools;\n        delete request.tool_choice;\n        return await this.sendRequest(request);\n      }\n      throw error;\n    }\n  }\n  \n  private isToolNotSupportedError(error: any): boolean {\n    return error.status === 404 || \n           error.message?.includes('tool') ||\n           error.message?.includes('function_call');\n  }\n}\n```",
        "testStrategy": "Test with various custom API endpoints:\n1. OpenAI-compatible APIs with/without tool support\n2. Anthropic-compatible APIs\n3. Generic REST APIs\n4. Verify graceful degradation when tools aren't supported\n5. Test error detection patterns for different API formats",
        "priority": "high",
        "dependencies": [
          22
        ],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 25,
        "title": "Fix ToolGroupMessage Duplicate Rendering",
        "description": "Implement state tracking and memoization in ToolGroupMessage component to prevent duplicate tool response displays",
        "details": "Update `/packages/cli/src/ui/components/messages/ToolGroupMessage.tsx`:\n\n```typescript\nimport React, { useMemo, useCallback, useRef } from 'react';\nimport { Box, Text } from 'ink';\n\ninterface ToolGroupMessageProps {\n  tools: ToolCall[];\n  results: ToolResult[];\n  messageId: string;\n}\n\nexport const ToolGroupMessage = React.memo(({ tools, results, messageId }: ToolGroupMessageProps) => {\n  // Track rendered tool IDs to prevent duplicates\n  const renderedToolIds = useRef(new Set<string>());\n  \n  // Generate unique keys for each tool\n  const getToolKey = useCallback((tool: ToolCall) => {\n    return `${messageId}-${tool.id}-${tool.name}`;\n  }, [messageId]);\n  \n  // Filter out already rendered tools\n  const uniqueTools = useMemo(() => {\n    return tools.filter(tool => {\n      const key = getToolKey(tool);\n      if (renderedToolIds.current.has(key)) {\n        return false;\n      }\n      renderedToolIds.current.add(key);\n      return true;\n    });\n  }, [tools, getToolKey]);\n  \n  // Match results to unique tools\n  const toolsWithResults = useMemo(() => {\n    return uniqueTools.map(tool => ({\n      tool,\n      result: results.find(r => r.toolCallId === tool.id)\n    }));\n  }, [uniqueTools, results]);\n  \n  if (toolsWithResults.length === 0) {\n    return null;\n  }\n  \n  return (\n    <Box flexDirection=\"column\" marginY={1}>\n      {toolsWithResults.map(({ tool, result }) => (\n        <Box key={getToolKey(tool)} flexDirection=\"column\" marginBottom={1}>\n          <Text color=\"cyan\">🔧 {tool.name}</Text>\n          <Box marginLeft={2}>\n            <Text dimColor>Input: {JSON.stringify(tool.arguments)}</Text>\n          </Box>\n          {result && (\n            <Box marginLeft={2}>\n              <Text color=\"green\">Result: {result.output}</Text>\n            </Box>\n          )}\n        </Box>\n      ))}\n    </Box>\n  );\n}, (prevProps, nextProps) => {\n  // Custom comparison to prevent unnecessary re-renders\n  return prevProps.messageId === nextProps.messageId &&\n         prevProps.tools.length === nextProps.tools.length &&\n         prevProps.results.length === nextProps.results.length;\n});\n```",
        "testStrategy": "Component tests should verify:\n1. Each tool is rendered exactly once\n2. Re-renders don't duplicate existing tools\n3. New tools are properly added\n4. Results are correctly matched to tools\n5. Performance with many rapid tool calls\n\nUse React Testing Library to simulate multiple render cycles",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 26,
        "title": "Implement Streaming JSON Buffer System",
        "description": "Create a robust buffering system to handle incomplete JSON chunks in streaming responses",
        "details": "Create `/packages/core/src/utils/streamingJsonBuffer.ts`:\n\n```typescript\nexport class StreamingJsonBuffer {\n  private buffer: string = '';\n  private depth: number = 0;\n  private inString: boolean = false;\n  private escapeNext: boolean = false;\n  \n  addChunk(chunk: string): string[] {\n    this.buffer += chunk;\n    const completeObjects: string[] = [];\n    let start = 0;\n    \n    for (let i = 0; i < this.buffer.length; i++) {\n      const char = this.buffer[i];\n      const prevChar = i > 0 ? this.buffer[i - 1] : '';\n      \n      // Handle escape sequences\n      if (this.escapeNext) {\n        this.escapeNext = false;\n        continue;\n      }\n      \n      if (char === '\\\\' && this.inString) {\n        this.escapeNext = true;\n        continue;\n      }\n      \n      // Track string boundaries\n      if (char === '\"' && !this.escapeNext) {\n        this.inString = !this.inString;\n        continue;\n      }\n      \n      // Only count braces outside of strings\n      if (!this.inString) {\n        if (char === '{') {\n          if (this.depth === 0) {\n            start = i;\n          }\n          this.depth++;\n        } else if (char === '}') {\n          this.depth--;\n          if (this.depth === 0) {\n            // Found complete object\n            const jsonStr = this.buffer.substring(start, i + 1);\n            try {\n              JSON.parse(jsonStr); // Validate\n              completeObjects.push(jsonStr);\n            } catch (e) {\n              // Invalid JSON, keep buffering\n            }\n          }\n        }\n      }\n    }\n    \n    // Remove processed objects from buffer\n    if (completeObjects.length > 0) {\n      const lastEnd = this.buffer.lastIndexOf('}') + 1;\n      this.buffer = this.buffer.substring(lastEnd);\n    }\n    \n    return completeObjects;\n  }\n  \n  flush(): string | null {\n    if (this.buffer.trim()) {\n      try {\n        JSON.parse(this.buffer);\n        return this.buffer;\n      } catch (e) {\n        return null;\n      }\n    }\n    return null;\n  }\n}\n```",
        "testStrategy": "Unit tests should cover:\n1. Complete JSON objects are extracted correctly\n2. Partial objects are buffered\n3. Multiple objects in one chunk\n4. Objects split across chunks\n5. Escaped quotes and special characters\n6. Nested objects and arrays\n7. Invalid JSON is handled gracefully",
        "priority": "medium",
        "dependencies": [],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 27,
        "title": "Integrate JSON Buffer with OpenRouter Streaming",
        "description": "Update OpenRouterContentGenerator to use the JSON buffer for streaming responses",
        "details": "Modify streaming logic in `/packages/core/src/providers/openRouterContentGenerator.ts`:\n\n```typescript\nimport { StreamingJsonBuffer } from '../utils/streamingJsonBuffer';\n\nprivate async *handleStreamingResponse(response: Response) {\n  const reader = response.body?.getReader();\n  if (!reader) throw new Error('No response body');\n  \n  const decoder = new TextDecoder();\n  const jsonBuffer = new StreamingJsonBuffer();\n  let timeoutId: NodeJS.Timeout;\n  \n  try {\n    while (true) {\n      // Set timeout for incomplete chunks\n      timeoutId = setTimeout(() => {\n        console.warn('Streaming timeout - flushing buffer');\n      }, 5000);\n      \n      const { done, value } = await reader.read();\n      clearTimeout(timeoutId);\n      \n      if (done) {\n        // Try to flush any remaining data\n        const remaining = jsonBuffer.flush();\n        if (remaining) {\n          yield this.parseStreamingLine(remaining);\n        }\n        break;\n      }\n      \n      const chunk = decoder.decode(value, { stream: true });\n      const lines = chunk.split('\\n');\n      \n      for (const line of lines) {\n        if (line.startsWith('data: ')) {\n          const data = line.slice(6);\n          if (data === '[DONE]') {\n            return;\n          }\n          \n          // Use buffer for JSON parsing\n          const completeObjects = jsonBuffer.addChunk(data);\n          for (const jsonStr of completeObjects) {\n            try {\n              const parsed = JSON.parse(jsonStr);\n              yield parsed;\n            } catch (e) {\n              console.error('Error parsing streaming JSON:', e);\n              // Continue processing other chunks\n            }\n          }\n        }\n      }\n    }\n  } finally {\n    reader.releaseLock();\n    clearTimeout(timeoutId!);\n  }\n}\n```",
        "testStrategy": "Integration tests should verify:\n1. Normal streaming works without regression\n2. Malformed JSON chunks are handled\n3. Slow connections don't cause data loss\n4. Timeout mechanism works correctly\n5. Memory usage remains stable with long streams\n6. Test with real API responses from OpenRouter",
        "priority": "medium",
        "dependencies": [
          26
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 28,
        "title": "Add Enhanced Error Handling for Tool Errors",
        "description": "Implement comprehensive error handling for tool-related API errors across all content generators",
        "details": "Create `/packages/core/src/providers/errors/toolErrors.ts`:\n\n```typescript\nexport class ToolNotSupportedError extends Error {\n  constructor(\n    public modelId: string,\n    public provider: string,\n    public originalError?: any\n  ) {\n    super(`Model ${modelId} does not support tool/function calling`);\n    this.name = 'ToolNotSupportedError';\n  }\n}\n\nexport class ToolErrorHandler {\n  static isToolError(error: any): boolean {\n    if (!error) return false;\n    \n    const errorPatterns = [\n      /no endpoints found.*tool/i,\n      /function.*not supported/i,\n      /tools.*not available/i,\n      /invalid.*tool.*request/i\n    ];\n    \n    const errorMessage = error.message || error.toString();\n    return errorPatterns.some(pattern => pattern.test(errorMessage));\n  }\n  \n  static async handleWithRetry<T>(\n    request: () => Promise<T>,\n    removeTools: () => void,\n    modelId: string\n  ): Promise<T> {\n    try {\n      return await request();\n    } catch (error) {\n      if (this.isToolError(error)) {\n        console.warn(`Tool error detected for ${modelId}, retrying without tools...`);\n        removeTools();\n        \n        // Update capability registry to remember this\n        modelCapabilityRegistry.updateCapability(modelId, { supportsTools: false });\n        \n        // Retry request\n        return await request();\n      }\n      throw error;\n    }\n  }\n  \n  static formatUserFriendlyError(error: any): string {\n    if (error instanceof ToolNotSupportedError) {\n      return `The selected model (${error.modelId}) doesn't support tool calling. ` +\n             `The request will proceed without tools.`;\n    }\n    \n    if (this.isToolError(error)) {\n      return 'Tool calling is not available for this model. Continuing without tools...';\n    }\n    \n    return error.message || 'An unexpected error occurred';\n  }\n}\n```\n\nUpdate all content generators to use this handler.",
        "testStrategy": "Test error handling with:\n1. Various error message formats from different providers\n2. Network errors vs API errors\n3. Retry logic success/failure scenarios\n4. User-friendly error message generation\n5. Registry updates after errors\n6. Performance impact of retries",
        "priority": "medium",
        "dependencies": [
          23,
          24
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 29,
        "title": "Update Provider Types for Tool Support",
        "description": "Extend provider type definitions to include tool capability information",
        "details": "Update `/packages/core/src/providers/types.ts`:\n\n```typescript\nexport interface ModelInfo {\n  id: string;\n  name: string;\n  provider: string;\n  contextLength: number;\n  capabilities: ModelCapabilities;\n  pricing?: ModelPricing;\n}\n\nexport interface ModelCapabilities {\n  chat: boolean;\n  completion: boolean;\n  tools: boolean;\n  vision: boolean;\n  streaming: boolean;\n  functionCalling: boolean; // Legacy, maps to tools\n}\n\nexport interface ProviderConfig {\n  name: string;\n  apiKey?: string;\n  baseUrl?: string;\n  models: ModelInfo[];\n  defaultModel?: string;\n  supportsTools?: boolean; // Provider-level default\n}\n\n// Extend existing interfaces\nexport interface GenerateOptions {\n  model?: string;\n  temperature?: number;\n  maxTokens?: number;\n  tools?: Tool[];\n  toolChoice?: 'auto' | 'none' | ToolChoice;\n  // New field for override\n  forceNoTools?: boolean;\n}\n\nexport interface ContentGenerator {\n  generateContent(\n    messages: Message[],\n    options: GenerateOptions\n  ): Promise<GenerateResponse>;\n  \n  // New method\n  getModelCapabilities(modelId: string): ModelCapabilities;\n}\n```",
        "testStrategy": "Type checking should verify:\n1. All existing code compiles with new types\n2. Optional fields don't break existing implementations\n3. Type inference works correctly\n4. Documentation generation includes new fields\n5. Backwards compatibility is maintained",
        "priority": "low",
        "dependencies": [
          22
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 30,
        "title": "Implement Capability Caching with Persistence",
        "description": "Add persistent caching for model capabilities to avoid repeated lookups and API calls",
        "details": "Extend `/packages/core/src/providers/modelCapabilities.ts`:\n\n```typescript\nimport { promises as fs } from 'fs';\nimport path from 'path';\nimport os from 'os';\n\nexport class PersistentModelCapabilityCache {\n  private cacheDir: string;\n  private cacheFile: string;\n  private memoryCache: Map<string, ModelCapability>;\n  private saveDebounceTimer?: NodeJS.Timeout;\n  \n  constructor() {\n    this.cacheDir = path.join(os.homedir(), '.sport-cli', 'cache');\n    this.cacheFile = path.join(this.cacheDir, 'model-capabilities.json');\n    this.memoryCache = new Map();\n    this.loadCache();\n  }\n  \n  private async loadCache() {\n    try {\n      await fs.mkdir(this.cacheDir, { recursive: true });\n      const data = await fs.readFile(this.cacheFile, 'utf-8');\n      const cached = JSON.parse(data);\n      \n      // Validate cache entries\n      for (const [key, value] of Object.entries(cached)) {\n        if (this.isValidCacheEntry(value)) {\n          this.memoryCache.set(key, value as ModelCapability);\n        }\n      }\n    } catch (error) {\n      // Cache doesn't exist or is invalid, start fresh\n      console.debug('No valid cache found, starting fresh');\n    }\n  }\n  \n  private isValidCacheEntry(entry: any): boolean {\n    if (!entry || typeof entry !== 'object') return false;\n    if (!entry.lastUpdated) return false;\n    \n    // Cache entries expire after 7 days\n    const expiryTime = 7 * 24 * 60 * 60 * 1000;\n    const age = Date.now() - new Date(entry.lastUpdated).getTime();\n    \n    return age < expiryTime;\n  }\n  \n  async set(modelId: string, capability: ModelCapability) {\n    this.memoryCache.set(modelId, {\n      ...capability,\n      lastUpdated: new Date()\n    });\n    \n    // Debounce saves to avoid excessive disk writes\n    if (this.saveDebounceTimer) {\n      clearTimeout(this.saveDebounceTimer);\n    }\n    \n    this.saveDebounceTimer = setTimeout(() => {\n      this.saveCache();\n    }, 1000);\n  }\n  \n  private async saveCache() {\n    try {\n      const data = Object.fromEntries(this.memoryCache);\n      await fs.writeFile(\n        this.cacheFile,\n        JSON.stringify(data, null, 2),\n        'utf-8'\n      );\n    } catch (error) {\n      console.error('Failed to save capability cache:', error);\n    }\n  }\n  \n  get(modelId: string): ModelCapability | undefined {\n    return this.memoryCache.get(modelId);\n  }\n  \n  clear() {\n    this.memoryCache.clear();\n    fs.unlink(this.cacheFile).catch(() => {});\n  }\n}\n```",
        "testStrategy": "Test caching functionality:\n1. Cache persistence across application restarts\n2. Cache expiry after 7 days\n3. Debounced saving to prevent disk thrashing\n4. Graceful handling of corrupted cache files\n5. Performance improvement measurements\n6. Concurrent access handling",
        "priority": "low",
        "dependencies": [
          22
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 31,
        "title": "Create Comprehensive Test Suite",
        "description": "Develop unit and integration tests for all bug fixes and new functionality",
        "details": "Create test files for each component:\n\n1. `/packages/core/src/providers/__tests__/modelCapabilities.test.ts`:\n```typescript\ndescribe('ModelCapabilityRegistry', () => {\n  it('should identify GPT-4 as supporting tools', () => {\n    expect(registry.supportsTools('gpt-4')).toBe(true);\n  });\n  \n  it('should identify Grok as not supporting tools', () => {\n    expect(registry.supportsTools('grok-beta')).toBe(false);\n  });\n  \n  it('should cache lookups for performance', () => {\n    const start = performance.now();\n    for (let i = 0; i < 1000; i++) {\n      registry.supportsTools('gpt-4');\n    }\n    const duration = performance.now() - start;\n    expect(duration).toBeLessThan(10); // Should be very fast\n  });\n});\n```\n\n2. `/packages/cli/src/ui/components/messages/__tests__/ToolGroupMessage.test.tsx`:\n```typescript\ndescribe('ToolGroupMessage', () => {\n  it('should not render duplicate tools', () => {\n    const tools = [{ id: '1', name: 'test', arguments: {} }];\n    const { rerender } = render(<ToolGroupMessage tools={tools} />);\n    \n    // Re-render with same tools\n    rerender(<ToolGroupMessage tools={tools} />);\n    \n    expect(screen.getAllByText('test')).toHaveLength(1);\n  });\n});\n```\n\n3. Integration tests in `/packages/core/src/providers/__tests__/integration.test.ts`:\n```typescript\ndescribe('Provider Integration', () => {\n  it('should handle model without tool support gracefully', async () => {\n    const generator = new OpenRouterContentGenerator();\n    const result = await generator.generateContent(\n      [{ role: 'user', content: 'Hello' }],\n      { model: 'grok-beta', tools: [mockTool] }\n    );\n    \n    expect(result).toBeDefined();\n    // Should not throw error\n  });\n});\n```",
        "testStrategy": "Test coverage should include:\n1. Unit tests for each new class/function\n2. Integration tests for API interactions\n3. Component tests for UI changes\n4. End-to-end tests for complete workflows\n5. Performance benchmarks\n6. Edge case handling\n7. Mock API responses for reliability",
        "priority": "medium",
        "dependencies": [
          22,
          23,
          24,
          25,
          26,
          27,
          28
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 32,
        "title": "Add Logging and Debugging Infrastructure",
        "description": "Implement comprehensive logging for debugging tool-related issues in production",
        "details": "Create `/packages/core/src/utils/logger.ts`:\n\n```typescript\nexport enum LogLevel {\n  ERROR = 0,\n  WARN = 1,\n  INFO = 2,\n  DEBUG = 3,\n  TRACE = 4\n}\n\nexport class Logger {\n  private static instance: Logger;\n  private level: LogLevel;\n  private logFile?: string;\n  \n  private constructor() {\n    this.level = this.getLogLevelFromEnv();\n    if (process.env.SPORT_CLI_LOG_FILE) {\n      this.logFile = process.env.SPORT_CLI_LOG_FILE;\n    }\n  }\n  \n  static getInstance(): Logger {\n    if (!this.instance) {\n      this.instance = new Logger();\n    }\n    return this.instance;\n  }\n  \n  private getLogLevelFromEnv(): LogLevel {\n    const level = process.env.SPORT_CLI_LOG_LEVEL?.toUpperCase();\n    return LogLevel[level as keyof typeof LogLevel] || LogLevel.INFO;\n  }\n  \n  private formatMessage(level: string, category: string, message: string, data?: any): string {\n    const timestamp = new Date().toISOString();\n    const dataStr = data ? ` ${JSON.stringify(data)}` : '';\n    return `[${timestamp}] [${level}] [${category}] ${message}${dataStr}`;\n  }\n  \n  private log(level: LogLevel, category: string, message: string, data?: any) {\n    if (level > this.level) return;\n    \n    const levelName = LogLevel[level];\n    const formatted = this.formatMessage(levelName, category, message, data);\n    \n    // Console output\n    if (level === LogLevel.ERROR) {\n      console.error(formatted);\n    } else if (level === LogLevel.WARN) {\n      console.warn(formatted);\n    } else {\n      console.log(formatted);\n    }\n    \n    // File output\n    if (this.logFile) {\n      fs.appendFileSync(this.logFile, formatted + '\\n');\n    }\n  }\n  \n  error(category: string, message: string, error?: any) {\n    this.log(LogLevel.ERROR, category, message, {\n      error: error?.message || error,\n      stack: error?.stack\n    });\n  }\n  \n  warn(category: string, message: string, data?: any) {\n    this.log(LogLevel.WARN, category, message, data);\n  }\n  \n  info(category: string, message: string, data?: any) {\n    this.log(LogLevel.INFO, category, message, data);\n  }\n  \n  debug(category: string, message: string, data?: any) {\n    this.log(LogLevel.DEBUG, category, message, data);\n  }\n}\n\nexport const logger = Logger.getInstance();\n```\n\nIntegrate throughout codebase:\n```typescript\n// In OpenRouterContentGenerator\nlogger.debug('OpenRouter', 'Checking tool support', { modelId, hasTools: !!options.tools });\nlogger.warn('OpenRouter', 'Model doesn\\'t support tools, removing from request', { modelId });\n```",
        "testStrategy": "Verify logging functionality:\n1. Log levels filter correctly\n2. File output works when configured\n3. Performance impact is minimal\n4. Sensitive data is not logged\n5. Log rotation works (if implemented)\n6. Structured logging format is parseable",
        "priority": "low",
        "dependencies": [
          28
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 33,
        "title": "Performance Testing and Optimization",
        "description": "Conduct performance testing and optimize any bottlenecks introduced by the bug fixes",
        "details": "Create performance benchmarks in `/packages/core/src/__benchmarks__/`:\n\n```typescript\n// capability-lookup.bench.ts\nimport { Bench } from 'tinybench';\nimport { modelCapabilityRegistry } from '../providers/modelCapabilities';\n\nconst bench = new Bench({ time: 100 });\n\nbench\n  .add('capability lookup - cached', () => {\n    modelCapabilityRegistry.supportsTools('gpt-4');\n  })\n  .add('capability lookup - uncached', () => {\n    modelCapabilityRegistry.supportsTools(`model-${Math.random()}`);\n  })\n  .add('bulk lookups - 100 models', () => {\n    for (let i = 0; i < 100; i++) {\n      modelCapabilityRegistry.supportsTools('gpt-4');\n    }\n  });\n\n// streaming-buffer.bench.ts\nimport { StreamingJsonBuffer } from '../utils/streamingJsonBuffer';\n\nbench\n  .add('parse complete JSON', () => {\n    const buffer = new StreamingJsonBuffer();\n    buffer.addChunk('{\"test\": \"data\"}');\n  })\n  .add('parse fragmented JSON', () => {\n    const buffer = new StreamingJsonBuffer();\n    buffer.addChunk('{\"te');\n    buffer.addChunk('st\": \"da');\n    buffer.addChunk('ta\"}');\n  });\n```\n\nOptimization targets:\n1. Capability lookups < 1ms\n2. JSON buffering overhead < 5%\n3. Memory usage stable over time\n4. No memory leaks in streaming\n5. UI rendering < 16ms per frame",
        "testStrategy": "Performance testing approach:\n1. Establish baseline metrics before changes\n2. Run benchmarks after each optimization\n3. Profile memory usage during streaming\n4. Test with large tool response payloads\n5. Simulate high-frequency tool calls\n6. Monitor for memory leaks over extended sessions\n7. Use Chrome DevTools for UI performance profiling",
        "priority": "low",
        "dependencies": [
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 34,
        "title": "Implement Debug Infrastructure and Logging Framework",
        "description": "Create comprehensive debug infrastructure with structured logging, correlation IDs, and trace points to enable troubleshooting of tool execution flow across all providers",
        "details": "1. Add --debug-tools CLI flag using commander.js or yargs\n2. Implement structured logging with winston v3.11+ or pino v8.16+:\n   - Create logger factory with log levels (debug, info, warn, error)\n   - Add correlation ID generation using uuid v9.0.1\n   - Implement context propagation for request tracking\n3. Create debug trace points:\n   - Provider detection: log provider type and raw response format\n   - Chunk reception: log each streaming chunk with timestamp\n   - Tool call detection: log when tool calls are identified\n   - Normalization: log before/after transformation\n   - Execution: log tool execution start/end with timing\n4. Add performance timing using perf_hooks:\n   ```javascript\n   const { performance } = require('perf_hooks');\n   const start = performance.now();\n   // ... operation ...\n   logger.debug(`Operation took ${performance.now() - start}ms`);\n   ```\n5. Implement log rotation and filtering to prevent log bloat",
        "testStrategy": "1. Unit tests for logger configuration and correlation ID generation\n2. Integration tests verifying debug output contains expected trace points\n3. Performance tests ensuring logging adds <10ms overhead\n4. Manual verification that --debug-tools flag produces actionable output",
        "priority": "high",
        "dependencies": [],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Add --debug-tools CLI Flag",
            "description": "Integrate a --debug-tools flag into the CLI using commander.js or yargs to enable or disable debug infrastructure and logging features at runtime.",
            "dependencies": [],
            "details": "Implement the CLI flag so that when enabled, it activates verbose logging, trace points, and performance timing throughout the tool execution flow.",
            "status": "pending",
            "testStrategy": "Verify the flag toggles debug output as expected in both enabled and disabled states using unit and integration tests."
          },
          {
            "id": 2,
            "title": "Implement Structured Logging with Correlation IDs",
            "description": "Set up structured logging using winston v3.11+ or pino v8.16+, including a logger factory with log levels and correlation ID generation using uuid v9.0.1.",
            "dependencies": [
              "34.1"
            ],
            "details": "Create a logger factory supporting debug, info, warn, and error levels. Generate a unique correlation ID for each request and propagate it through the execution context for end-to-end request tracking.",
            "status": "pending",
            "testStrategy": "Unit test logger configuration, log level filtering, and correlation ID generation. Integration test that logs include correlation IDs and are correctly propagated."
          },
          {
            "id": 3,
            "title": "Add Debug Trace Points and Contextual Logging",
            "description": "Instrument key execution points with structured logs: provider detection, chunk reception, tool call detection, normalization, and execution timing.",
            "dependencies": [
              "34.2"
            ],
            "details": "Log provider type and raw response, each streaming chunk with timestamp, tool call detection, normalization steps (before/after), and tool execution start/end with timing. Ensure all logs include correlation IDs for traceability.",
            "status": "pending",
            "testStrategy": "Integration tests to verify logs are emitted at all trace points with correct context and correlation IDs. Manual verification that logs provide actionable insights."
          },
          {
            "id": 4,
            "title": "Implement Performance Timing, Log Rotation, and Filtering",
            "description": "Integrate performance timing using perf_hooks, and add log rotation and filtering to prevent log bloat and maintain performance.",
            "dependencies": [
              "34.3"
            ],
            "details": "Use perf_hooks to measure and log operation durations. Configure log rotation to archive or delete old logs, and implement filtering to control log verbosity and prevent excessive log growth.",
            "status": "pending",
            "testStrategy": "Performance tests to ensure logging overhead is <10ms. Unit and integration tests for log rotation and filtering. Manual checks that log files do not grow unbounded."
          }
        ]
      },
      {
        "id": 35,
        "title": "Design and Implement Provider Normalization Layer",
        "description": "Create a provider-agnostic normalization layer that transforms various provider response formats into a unified intermediate representation for consistent tool call handling",
        "details": "1. Define TypeScript interfaces for normalized structures:\n   ```typescript\n   interface NormalizedChunk {\n     content?: string;\n     toolCalls?: ToolCall[];\n     isComplete: boolean;\n     metadata: { provider: string; model: string; timestamp: number; }\n   }\n   \n   interface ToolCall {\n     id: string;\n     name: string;\n     arguments: Record<string, any>;\n     status: 'pending' | 'partial' | 'complete';\n   }\n   ```\n2. Implement provider detection using response signatures:\n   - OpenAI: Check for choices[0].delta.function_call or tool_calls\n   - Anthropic: Check for content array with type='tool_use'\n   - Gemini: Check for functionCall or candidates[0].content.parts\n   - OpenRouter: Detect underlying provider from headers/metadata\n3. Create normalization functions for each provider:\n   - OpenAI: Handle both legacy function_call and modern tool_calls formats\n   - Anthropic: Extract tool_use blocks from content array\n   - Gemini: Map existing functionCall to normalized format\n   - Generic fallback for unknown providers\n4. Implement JSON parsing with error recovery for partial chunks:\n   ```javascript\n   const parsePartialJSON = (str) => {\n     try {\n       return JSON.parse(str);\n     } catch {\n       // Attempt to complete partial JSON\n       const completed = str + '}'.repeat((str.match(/{/g) || []).length - (str.match(/}/g) || []).length);\n       return JSON.parse(completed);\n     }\n   };\n   ```\n5. Add provider version detection for future compatibility",
        "testStrategy": "1. Unit tests with real provider response samples for each format\n2. Tests for partial JSON handling and error cases\n3. Property-based testing with fast-check to verify normalization invariants\n4. Integration tests with mock provider responses",
        "priority": "high",
        "dependencies": [
          34
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Define Normalized Data Interfaces",
            "description": "Establish TypeScript interfaces for the unified normalized structures, including NormalizedChunk and ToolCall, to serve as the foundation for all provider normalization.",
            "dependencies": [],
            "details": "Create and document the NormalizedChunk and ToolCall interfaces, ensuring they capture all necessary fields for downstream tool call handling and metadata tracking.",
            "status": "pending",
            "testStrategy": "Write unit tests to validate type correctness and ensure all required fields are present and properly typed."
          },
          {
            "id": 2,
            "title": "Implement Provider Detection Logic",
            "description": "Develop logic to accurately detect the originating provider and response format using unique response signatures and metadata.",
            "dependencies": [
              "35.1"
            ],
            "details": "Implement detection for OpenAI (choices.delta.function_call/tool_calls), Anthropic (content array with type='tool_use'), Gemini (functionCall or candidates.content.parts), and OpenRouter (headers/metadata).",
            "status": "pending",
            "testStrategy": "Test detection logic with a variety of real and synthetic provider responses, including ambiguous and malformed cases."
          },
          {
            "id": 3,
            "title": "Create Provider-Specific Normalization Functions",
            "description": "Develop normalization functions for each supported provider to transform their native response formats into the unified NormalizedChunk structure.",
            "dependencies": [
              "35.1",
              "35.2"
            ],
            "details": "Implement normalization for OpenAI (legacy function_call and tool_calls), Anthropic (tool_use blocks), Gemini (functionCall mapping), and a generic fallback for unknown providers.",
            "status": "pending",
            "testStrategy": "Write unit and integration tests using real provider samples to verify correct mapping and invariants across all supported formats."
          },
          {
            "id": 4,
            "title": "Handle Edge Cases and Partial JSON Parsing",
            "description": "Implement robust handling for edge cases such as partial or malformed JSON in streaming chunks, ensuring error recovery and data integrity.",
            "dependencies": [
              "35.3"
            ],
            "details": "Develop and test a partial JSON parser with error recovery logic to handle incomplete data, and ensure normalization functions gracefully handle unknown or unexpected formats.",
            "status": "pending",
            "testStrategy": "Test with intentionally truncated and malformed JSON responses, verifying recovery and fallback behavior."
          },
          {
            "id": 5,
            "title": "Add Provider Version and Compatibility Detection",
            "description": "Extend normalization logic to detect provider versions and adapt to future format changes for forward compatibility.",
            "dependencies": [
              "35.2",
              "35.3"
            ],
            "details": "Implement version detection using response metadata or headers, and design normalization functions to branch logic based on detected version.",
            "status": "pending",
            "testStrategy": "Simulate versioned responses and verify correct branching and compatibility handling in normalization."
          }
        ]
      },
      {
        "id": 36,
        "title": "Refactor Streaming Buffer Management System",
        "description": "Replace string-based accumulation with a structured buffer system that properly tracks and assembles tool calls across streaming chunks without losing data",
        "details": "1. Implement structured streaming buffer:\n   ```typescript\n   class StreamingBuffer {\n     private contentBuffer: string[] = [];\n     private toolCallBuffer: Map<string, Partial<ToolCall>> = new Map();\n     private chunkHistory: NormalizedChunk[] = [];\n     \n     addChunk(chunk: NormalizedChunk): void {\n       // Accumulate content separately from tool calls\n       if (chunk.content) this.contentBuffer.push(chunk.content);\n       \n       // Merge partial tool calls\n       chunk.toolCalls?.forEach(tc => {\n         const existing = this.toolCallBuffer.get(tc.id);\n         this.toolCallBuffer.set(tc.id, { ...existing, ...tc });\n       });\n     }\n     \n     getCompleteToolCalls(): ToolCall[] {\n       return Array.from(this.toolCallBuffer.values())\n         .filter(tc => tc.status === 'complete');\n     }\n   }\n   ```\n2. Handle chunk boundaries for split JSON:\n   - Detect incomplete JSON at chunk end\n   - Buffer incomplete data for next chunk\n   - Use state machine for parsing context\n3. Implement backpressure handling:\n   - Monitor buffer size and apply flow control\n   - Emit warnings if buffer grows too large\n4. Add chunk reassembly for split tool calls:\n   - Track partial argument accumulation\n   - Handle nested JSON structures\n   - Support arrays split across chunks\n5. Create separate display and processing pipelines:\n   - Display pipeline: immediate content rendering\n   - Processing pipeline: buffered tool call assembly",
        "testStrategy": "1. Unit tests simulating various chunk splitting scenarios\n2. Stress tests with rapid small chunks to verify no data loss\n3. Integration tests with real streaming responses\n4. Memory leak tests for long-running streams",
        "priority": "high",
        "dependencies": [
          35
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Design and Implement Structured Streaming Buffer Class",
            "description": "Create a robust StreamingBuffer class to replace string-based accumulation, enabling separate tracking of content and tool calls, and supporting partial tool call assembly across streaming chunks.",
            "dependencies": [],
            "details": "Define internal buffers for content and tool calls, implement methods for adding chunks, merging partial tool calls, and retrieving complete tool calls. Ensure the class supports extensibility for future buffer management needs.",
            "status": "pending",
            "testStrategy": "Develop unit tests to verify correct accumulation of content and tool calls, proper merging of partial tool calls, and accurate retrieval of complete tool calls."
          },
          {
            "id": 2,
            "title": "Implement Chunk Assembly and Boundary Handling Logic",
            "description": "Develop logic to detect and handle incomplete JSON or tool call data at chunk boundaries, ensuring no data loss and correct reassembly of split tool calls across multiple chunks.",
            "dependencies": [
              "36.1"
            ],
            "details": "Integrate a state machine or similar mechanism to track parsing context, buffer incomplete data, and reassemble tool calls or arguments that span multiple chunks. Support nested JSON structures and arrays split across chunks.",
            "status": "pending",
            "testStrategy": "Create unit and integration tests simulating various chunk splitting scenarios, including nested and split JSON, to verify correct reassembly and data integrity."
          },
          {
            "id": 3,
            "title": "Separate Content Display Pipeline from Tool Call Processing",
            "description": "Establish distinct pipelines for immediate content rendering and buffered tool call assembly, ensuring that user-facing content is displayed without delay while tool calls are processed reliably in the background.",
            "dependencies": [
              "36.1",
              "36.2"
            ],
            "details": "Implement logic to route content chunks directly to the display pipeline, while tool call data is buffered and assembled before processing. Ensure synchronization between pipelines where necessary.",
            "status": "pending",
            "testStrategy": "Write integration tests to confirm that content is rendered immediately and tool calls are processed only when fully assembled, with no cross-pipeline data leakage."
          },
          {
            "id": 4,
            "title": "Ensure Robust State Management for Partial Tool Calls",
            "description": "Implement comprehensive state management within the buffer system to accurately track, update, and finalize partial tool calls, preventing data loss or duplication during streaming.",
            "dependencies": [
              "36.1",
              "36.2",
              "36.3"
            ],
            "details": "Design mechanisms to monitor the state of each tool call, handle updates as new chunks arrive, and mark tool calls as complete only when all required data is present. Support error handling for malformed or incomplete tool calls.",
            "status": "pending",
            "testStrategy": "Develop unit and stress tests to verify correct state transitions, handling of edge cases, and resilience to malformed or incomplete tool call data."
          }
        ]
      },
      {
        "id": 37,
        "title": "Fix Tool Execution Flow and Promise Chain Management",
        "description": "Update the Turn class and execution flow to properly await tool executions, handle multiple concurrent calls, and ensure CLI doesn't exit before completion",
        "details": "1. Update Turn class to use normalized tool calls:\n   ```javascript\n   class Turn {\n     async processNormalizedResponse(response) {\n       const buffer = new StreamingBuffer();\n       \n       for await (const chunk of response) {\n         const normalized = normalizeProviderChunk(chunk);\n         buffer.addChunk(normalized);\n         \n         // Display content immediately\n         if (normalized.content) {\n           process.stdout.write(normalized.content);\n         }\n       }\n       \n       // Execute all complete tool calls\n       const toolCalls = buffer.getCompleteToolCalls();\n       if (toolCalls.length > 0) {\n         await this.executeToolCalls(toolCalls);\n       }\n     }\n     \n     async executeToolCalls(toolCalls) {\n       // Execute in parallel with concurrency limit\n       const results = await pLimit(5)(toolCalls.map(tc => \n         () => this.executeSingleTool(tc)\n       ));\n       return results;\n     }\n   }\n   ```\n2. Implement proper promise chain:\n   - Use Promise.allSettled for multiple tool calls\n   - Add timeout handling (30s default, configurable)\n   - Implement retry logic for transient failures\n3. Add graceful shutdown handling:\n   ```javascript\n   process.on('SIGINT', async () => {\n     console.log('\\nGracefully shutting down...');\n     await pendingExecutions.waitForAll();\n     process.exit(0);\n   });\n   ```\n4. Create execution tracking:\n   - Track pending executions globally\n   - Prevent CLI exit until all complete\n   - Add progress indicators for long-running tools\n5. Implement error boundaries:\n   - Catch and log tool execution errors\n   - Continue with remaining tools on failure\n   - Aggregate errors for final reporting",
        "testStrategy": "1. Unit tests for promise chain management and timeout handling\n2. Integration tests verifying CLI waits for tool completion\n3. Tests for concurrent execution limits and error handling\n4. Manual tests with slow/failing tools to verify graceful behavior",
        "priority": "high",
        "dependencies": [
          36
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Refactor Turn Class for Normalized Tool Calls",
            "description": "Update the Turn class to process and buffer normalized tool calls, ensuring that tool call extraction and streaming output are handled efficiently.",
            "dependencies": [],
            "details": "Modify the processNormalizedResponse method to use a streaming buffer for normalized chunks, extract complete tool calls, and prepare them for execution. Ensure immediate display of content and correct buffering of tool call data.",
            "status": "pending",
            "testStrategy": "Unit test the Turn class with various streaming responses to verify correct buffering, extraction, and immediate output of content."
          },
          {
            "id": 2,
            "title": "Implement Concurrent Tool Execution with Await Handling",
            "description": "Enable concurrent execution of multiple tool calls with proper await logic, ensuring all executions are completed before proceeding.",
            "dependencies": [
              "37.1"
            ],
            "details": "Use a concurrency limiter (e.g., pLimit) to execute tool calls in parallel, respecting a configurable concurrency cap. Ensure all executions are awaited so that the CLI does not proceed or exit prematurely.",
            "status": "pending",
            "testStrategy": "Integration test with multiple tool calls to verify concurrency limits, correct awaiting, and that all executions complete before CLI exit."
          },
          {
            "id": 3,
            "title": "Establish Robust Promise Chain Management",
            "description": "Refactor promise chains to use Promise.allSettled for multiple tool calls, add timeout handling, and implement retry logic for transient failures.",
            "dependencies": [
              "37.2"
            ],
            "details": "Ensure all asynchronous tool executions are managed with Promise.allSettled to capture both successes and failures. Add a default (configurable) timeout for each tool call and implement retry logic for transient errors, following best practices for promise chaining and error propagation[1][2][3][4].",
            "status": "pending",
            "testStrategy": "Unit and integration tests for promise chain correctness, timeout enforcement, and retry behavior under simulated transient failures."
          },
          {
            "id": 4,
            "title": "Add Graceful Shutdown and Execution Tracking",
            "description": "Implement global tracking of pending executions and graceful shutdown logic to prevent CLI exit before all tool calls complete.",
            "dependencies": [
              "37.3"
            ],
            "details": "Track all pending tool executions globally. Hook into process signals (e.g., SIGINT) to wait for all pending executions before exiting. Add progress indicators for long-running tool calls.",
            "status": "pending",
            "testStrategy": "Integration and manual tests to verify that the CLI waits for all executions on shutdown and displays progress for long-running tasks."
          },
          {
            "id": 5,
            "title": "Integrate Error Boundaries and Reporting",
            "description": "Add error boundaries to catch and log tool execution errors, continue processing remaining tools, and aggregate errors for final reporting.",
            "dependencies": [
              "37.4"
            ],
            "details": "Wrap tool execution logic in try/catch blocks or use .catch handlers to log errors without halting execution of other tools. Aggregate all errors and present a summary report after all executions complete.",
            "status": "pending",
            "testStrategy": "Unit and integration tests with failing tool calls to verify error logging, continued execution, and correct aggregation of error reports."
          }
        ]
      },
      {
        "id": 38,
        "title": "Implement Comprehensive Test Suite and Provider Validation",
        "description": "Create unit and integration tests covering all provider formats, edge cases, and end-to-end tool execution scenarios to ensure reliability across all supported AI providers",
        "details": "1. Unit test suite structure:\n   ```javascript\n   describe('Provider Normalization', () => {\n     describe('OpenAI', () => {\n       test('handles function_call format', () => {});\n       test('handles tool_calls array format', () => {});\n       test('handles streaming deltas', () => {});\n     });\n     \n     describe('Anthropic', () => {\n       test('extracts tool_use from content', () => {});\n       test('handles multiple tools in response', () => {});\n     });\n     \n     describe('Gemini', () => {\n       test('maintains backward compatibility', () => {});\n       test('handles functionCall format', () => {});\n     });\n   });\n   ```\n2. Integration test scenarios:\n   - Single tool call execution\n   - Multiple concurrent tool calls\n   - Tool calls split across chunks\n   - Error handling and recovery\n   - Timeout and cancellation\n3. Create test fixtures from real provider responses:\n   - Capture actual API responses for each provider\n   - Store as JSON fixtures for reproducible tests\n   - Include edge cases and error responses\n4. Implement provider mocks:\n   ```javascript\n   class MockProvider {\n     async *streamResponse(chunks) {\n       for (const chunk of chunks) {\n         await new Promise(r => setTimeout(r, 10));\n         yield chunk;\n       }\n     }\n   }\n   ```\n5. Add performance benchmarks:\n   - Measure latency impact of normalization\n   - Track memory usage during streaming\n   - Verify <100ms overhead requirement\n6. Create end-to-end test harness:\n   - Spawn CLI process programmatically\n   - Verify tool execution via side effects\n   - Test with real providers in CI (with API keys)",
        "testStrategy": "1. Run unit tests on every commit with 100% coverage target\n2. Integration tests in CI with mocked providers\n3. Nightly tests against real providers with test API keys\n4. Manual acceptance testing with each provider before release",
        "priority": "medium",
        "dependencies": [
          37
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Develop Unit Tests for Provider Normalization Functions",
            "description": "Create comprehensive unit tests for normalization logic of each supported provider (OpenAI, Anthropic, Gemini), ensuring all response formats and streaming behaviors are covered using real provider samples.",
            "dependencies": [],
            "details": "Implement test cases for each provider's unique response structures, including function_call, tool_calls, streaming deltas, and backward compatibility. Use real-world samples to validate normalization correctness.",
            "status": "pending",
            "testStrategy": "Run unit tests on every commit with a 100% coverage target. Validate against real and synthetic provider responses to ensure normalization logic is robust."
          },
          {
            "id": 2,
            "title": "Implement Integration Tests for End-to-End Tool Execution",
            "description": "Design and execute integration tests simulating end-to-end tool execution scenarios, including single and multiple tool calls, chunked responses, and error handling across all providers.",
            "dependencies": [
              "38.1"
            ],
            "details": "Programmatically invoke CLI processes and simulate tool execution flows. Test with both mocked and real providers to verify correct orchestration and side effects.",
            "status": "pending",
            "testStrategy": "Integration tests run in CI with mocked providers and nightly with real providers using test API keys. Verify tool execution outcomes and error recovery."
          },
          {
            "id": 3,
            "title": "Create Edge Case and Fault Injection Tests",
            "description": "Develop tests targeting malformed provider responses, network interruptions, timeouts, and cancellation scenarios to ensure robust error handling and recovery.",
            "dependencies": [
              "38.1"
            ],
            "details": "Simulate edge cases such as incomplete JSON, unexpected data types, and network failures. Validate that the system gracefully handles and recovers from these conditions.",
            "status": "pending",
            "testStrategy": "Automated tests inject faults and malformed data, verifying that errors are detected, logged, and do not cause system crashes or data corruption."
          },
          {
            "id": 4,
            "title": "Build and Maintain Test Fixtures with Real Provider Responses",
            "description": "Capture and curate a library of real API responses from each provider, including edge cases and error scenarios, to use as reproducible test fixtures.",
            "dependencies": [],
            "details": "Store provider responses as versioned JSON fixtures. Ensure fixtures cover all supported formats, edge cases, and error conditions for reliable and repeatable testing.",
            "status": "pending",
            "testStrategy": "Fixtures are validated for completeness and updated as provider APIs evolve. Tests using fixtures are run regularly to detect regressions."
          },
          {
            "id": 5,
            "title": "Establish Performance Benchmarks for Normalization and Streaming",
            "description": "Implement performance tests to measure latency and memory usage of normalization and streaming logic, ensuring overhead remains below defined thresholds.",
            "dependencies": [
              "38.1",
              "38.2"
            ],
            "details": "Benchmark normalization and streaming under various load conditions. Track metrics such as processing latency and memory consumption, verifying compliance with performance requirements (<100ms overhead).",
            "status": "pending",
            "testStrategy": "Automated performance benchmarks run in CI and on demand. Regression alerts are triggered if performance thresholds are exceeded."
          }
        ]
      },
      {
        "id": 39,
        "title": "Update CLI Branding from Gemini to Sport",
        "description": "Replace all references to 'Gemini CLI' with 'Sport CLI' throughout the codebase, starting with the yargs configuration in config.ts",
        "details": "1. Update packages/cli/src/config/config.ts:\n   - Change .scriptName('gemini') to .scriptName('sport')\n   - Update usage text from 'Gemini CLI - Launch an interactive CLI...' to 'Sport CLI - Launch an interactive CLI...'\n\n2. Search entire codebase for 'Gemini CLI' references:\n   ```bash\n   grep -r \"Gemini CLI\" packages/\n   grep -r \"gemini\" packages/ --include=\"*.ts\" --include=\"*.tsx\"\n   ```\n\n3. Update HTTP headers and referer URLs if any contain 'gemini'\n\n4. Ensure package.json bin field uses 'sport' command\n\n5. Update any documentation files that reference the old branding",
        "testStrategy": "1. Run 'sport --help' and verify it shows 'Sport CLI' in usage text\n2. Verify command invocation works with 'sport' command\n3. Check all error messages and prompts for correct branding\n4. Run automated tests to ensure no functionality is broken\n5. Manually test common workflows to verify branding consistency",
        "priority": "high",
        "dependencies": [],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 40,
        "title": "Implement Backward Compatibility for Configuration Directories",
        "description": "Ensure the CLI works with both existing .gemini/ and new .sport/ configuration directories to maintain backward compatibility",
        "details": "1. Locate configuration directory logic (likely in config initialization)\n\n2. Implement fallback mechanism:\n   ```typescript\n   const getConfigDir = (): string => {\n     const sportDir = path.join(os.homedir(), '.sport');\n     const geminiDir = path.join(os.homedir(), '.gemini');\n     \n     // Check if .sport exists first\n     if (fs.existsSync(sportDir)) {\n       return sportDir;\n     }\n     \n     // Fall back to .gemini for backward compatibility\n     if (fs.existsSync(geminiDir)) {\n       console.log('Using legacy .gemini directory. Consider migrating to .sport');\n       return geminiDir;\n     }\n     \n     // Create new .sport directory for new installations\n     fs.mkdirSync(sportDir, { recursive: true });\n     return sportDir;\n   };\n   ```\n\n3. Add migration helper (optional):\n   ```typescript\n   const migrateConfig = async (): Promise<void> => {\n     if (fs.existsSync(geminiDir) && !fs.existsSync(sportDir)) {\n       console.log('Migrating configuration from .gemini to .sport...');\n       await fs.promises.cp(geminiDir, sportDir, { recursive: true });\n     }\n   };\n   ```",
        "testStrategy": "1. Test with only .gemini/ directory present - verify it works\n2. Test with only .sport/ directory present - verify it works\n3. Test with both directories present - verify .sport takes precedence\n4. Test fresh installation - verify .sport/ is created\n5. Test configuration file reading/writing in both scenarios\n6. Verify any API keys and settings are properly loaded from either directory",
        "priority": "high",
        "dependencies": [
          39
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 41,
        "title": "Optimize Processing Animation Timing",
        "description": "Fix the delay between model response completion and processing animation appearance by optimizing the streaming buffer and UI update pipeline",
        "details": "1. Profile current implementation in streamingJsonBuffer.ts:\n   ```typescript\n   // Add timing logs to identify bottlenecks\n   console.time('buffer-accumulation');\n   // ... existing buffer logic\n   console.timeEnd('buffer-accumulation');\n   ```\n\n2. Implement immediate UI feedback:\n   ```typescript\n   // In the streaming handler\n   const handleStreamChunk = (chunk: string) => {\n     // Immediately trigger UI update\n     setProcessingState(true);\n     \n     // Process chunk asynchronously\n     requestAnimationFrame(() => {\n       bufferProcessor.addChunk(chunk);\n     });\n   };\n   ```\n\n3. Optimize buffer flushing:\n   ```typescript\n   class StreamingJsonBuffer {\n     private flushTimeout: NodeJS.Timeout | null = null;\n     \n     addChunk(chunk: string) {\n       this.buffer += chunk;\n       \n       // Immediate flush for complete JSON objects\n       if (this.isCompleteJson(this.buffer)) {\n         this.flush();\n       } else {\n         // Debounced flush for partial data\n         this.scheduleFlush();\n       }\n     }\n     \n     private scheduleFlush() {\n       if (this.flushTimeout) clearTimeout(this.flushTimeout);\n       this.flushTimeout = setTimeout(() => this.flush(), 16); // ~60fps\n     }\n   }\n   ```\n\n4. Consider using React's startTransition for non-urgent updates",
        "testStrategy": "1. Add performance markers to measure delay before and after optimization\n2. Test with various model providers (OpenAI, Anthropic, Google, Grok)\n3. Measure time between last model token and animation start (target < 100ms)\n4. Test with slow/fast network conditions\n5. Verify no memory leaks from rapid buffer updates\n6. Ensure streaming text still appears smoothly without stuttering",
        "priority": "medium",
        "dependencies": [],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 42,
        "title": "Investigate and Document Grok-4 Function Calling Behavior",
        "description": "Create comprehensive tests for Grok-4 function calling capabilities and document any limitations or special requirements",
        "details": "1. Create test suite for Grok-4:\n   ```typescript\n   // tests/grok4-function-calling.test.ts\n   describe('Grok-4 Function Calling', () => {\n     const testCases = [\n       {\n         prompt: 'What is the weather in Tokyo?',\n         expectedTool: 'get_weather',\n         params: { location: 'Tokyo' }\n       },\n       {\n         prompt: 'Search for information about TypeScript',\n         expectedTool: 'web_search',\n         params: { query: 'TypeScript' }\n       }\n     ];\n     \n     test.each(testCases)('should call $expectedTool', async ({ prompt, expectedTool, params }) => {\n       const response = await grok4.generate(prompt, { tools: availableTools });\n       // Verify tool_calls in response\n     });\n   });\n   ```\n\n2. Compare response formats:\n   ```typescript\n   // Log and compare responses from different models\n   const models = ['grok-4', 'gpt-4', 'claude-3'];\n   for (const model of models) {\n     console.log(`\\n=== ${model} Response Format ===`);\n     const response = await generateWithTools(model, testPrompt);\n     console.log(JSON.stringify(response, null, 2));\n   }\n   ```\n\n3. Test prompt engineering variations:\n   ```typescript\n   const promptVariations = [\n     // Standard\n     'What is the weather in Tokyo?',\n     // Explicit instruction\n     'Use the get_weather function to check Tokyo weather',\n     // System message emphasis\n     { system: 'You must use tools when appropriate', user: 'Weather in Tokyo?' }\n   ];\n   ```\n\n4. Check OpenRouter headers and model routing",
        "testStrategy": "1. Run function calling tests with multiple tools available\n2. Test single tool scenarios vs multiple tool scenarios\n3. Compare Grok-4 responses with GPT-4 and Claude for same prompts\n4. Test edge cases: ambiguous requests, multiple tool calls, invalid parameters\n5. Monitor OpenRouter API responses for any error codes or warnings\n6. Document findings in a GROK4_LIMITATIONS.md file\n7. Create integration test that runs periodically to detect behavior changes",
        "priority": "low",
        "dependencies": [],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 43,
        "title": "Update Documentation and Release Notes",
        "description": "Create comprehensive documentation for the fixes and update release notes to reflect all changes made to sport-cli",
        "details": "1. Update README.md:\n   - Change all references from gemini-cli to sport-cli\n   - Update installation instructions to use 'sport' command\n   - Add migration guide for existing users\n\n2. Create MIGRATION.md:\n   ```markdown\n   # Migrating from Gemini CLI to Sport CLI\n   \n   ## Configuration Directory\n   Sport CLI maintains backward compatibility with `.gemini/` directories.\n   To migrate:\n   1. Your existing configuration will work as-is\n   2. Optionally, rename ~/.gemini to ~/.sport\n   \n   ## Command Changes\n   - Old: `gemini chat`\n   - New: `sport chat`\n   ```\n\n3. Update CHANGELOG.md:\n   ```markdown\n   ## [Version] - 2025-07-21\n   \n   ### Changed\n   - Rebranded from Gemini CLI to Sport CLI\n   - Optimized processing animation timing (< 100ms delay)\n   \n   ### Fixed\n   - Processing animation delay issue\n   - CLI branding consistency\n   \n   ### Documented\n   - Grok-4 function calling behavior and limitations\n   ```\n\n4. Create or update API documentation for any changed interfaces\n\n5. Update package.json description and keywords",
        "testStrategy": "1. Review all documentation for accuracy and completeness\n2. Test all code examples in documentation\n3. Verify migration guide works for real user scenarios\n4. Check that all links in documentation are valid\n5. Ensure documentation is consistent with actual CLI behavior\n6. Have someone unfamiliar with the project follow the installation guide",
        "priority": "medium",
        "dependencies": [
          39,
          40,
          41,
          42
        ],
        "status": "pending",
        "subtasks": []
      }
    ],
    "metadata": {
      "created": "2025-07-07T18:56:28.419Z",
      "updated": "2025-07-21T15:53:07.711Z",
      "description": "Tasks for master context"
    }
  }
}